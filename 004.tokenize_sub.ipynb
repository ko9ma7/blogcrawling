{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:58:35.467228Z",
     "start_time": "2021-01-13T08:58:35.463228Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook,tqdm_pandas\n",
    "import itertools\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T09:00:34.816997Z",
     "start_time": "2021-01-13T09:00:34.805995Z"
    }
   },
   "outputs": [],
   "source": [
    "filter_word = pd.read_excel('filter_word.xlsx')['filter_word'].values\n",
    "path = 'new_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T08:58:35.506238Z",
     "start_time": "2021-01-13T08:58:35.503237Z"
    }
   },
   "outputs": [],
   "source": [
    "merge_file = dict()\n",
    "\n",
    "merge_file['광진'] = ['광진리','광진','광진해변','휴휴암'] #광진 +강원도 삭제\n",
    "merge_file['기사문'] = ['기사문리','기사문','기사문항','기사문해변']\n",
    "merge_file['낙산'] = ['낙산','낙산항','낙산해수욕장']\n",
    "merge_file['남애'] = ['남애리','남애','남애항'] #남애 +강원도 삭제\n",
    "merge_file['동호'] = ['동호','동호리','동호해변','동호해수욕장'] #동호 +강원도 삭제 \n",
    "merge_file['동산'] = ['동산','동산리','동산항','동산해수욕장'] #동산 +강원도 삭제\n",
    "merge_file['물치'] = ['물치리','물치','물치항']\n",
    "merge_file['수산'] = ['수산리','수산항'] #'수산' 제외\n",
    "merge_file['오산'] = ['오산','오산해수욕장','오산리'] #오산 +강원도 삭제\n",
    "merge_file['인구'] = ['인구리','죽도'] # '인구' 제외\n",
    "merge_file['전진'] = ['전진리','설악해수욕장','후진항']\n",
    "merge_file['하광정'] = ['하광정리','하조대','하조대해수욕장']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T09:01:22.957061Z",
     "start_time": "2021-01-13T09:01:22.953060Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def separate_merge(path):\n",
    "    '''파일분할되어 있는 결과를 종합해줌\n",
    "    경로를 입력하면 파일들이 분할되어 있는 폴더를 선택하고\n",
    "    폴더 내에 있는 파일들을 concat하여 폴더밖으로 배출\n",
    "    '''\n",
    "    file_list = os.listdir(f'./{path}/token/')\n",
    "    folder_list = []\n",
    "    #분할된 파일 탐색\n",
    "    for file in file_list:\n",
    "        if os.path.splitext(file)[1] != '.csv':\n",
    "            folder_list.append(file)\n",
    "            \n",
    "    #분할된 파일 통합\n",
    "    for folder in tqdm_notebook(folder_list):\n",
    "        file_list = os.listdir(f'./{path}/token/{folder}/')\n",
    "        all_df = pd.DataFrame()\n",
    "        for file in file_list:\n",
    "            read_file = pd.read_csv(f'./{path}/token/{folder}/{file}')\n",
    "            all_df = pd.concat([all_df,read_file])\n",
    "            size = all_df.shape[0]\n",
    "        all_df.to_csv(f'./{path}/token/{folder}_0~{size}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T09:00:48.480098Z",
     "start_time": "2021-01-13T09:00:48.472104Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def setting_file(path, merge_file):\n",
    "    '''결합해야하는 파일들을 결합 이후 필터링 진행\n",
    "    다양한 키워드를 하나의 keyword로 결합\n",
    "    이후 다시한번 필터링 진행\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    path(string) : 파일이 있는 경로\n",
    "    merge_file(dictionary) : 결합해야하는 keyword \n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    f'./output/token_통합/{key}_{total_len}.csv'의 형태로 파일저장\n",
    "    '''\n",
    "    os.makedirs(f'./{path}/token_통합',exist_ok=True)\n",
    "    \n",
    "    #대상 csv목록 생성\n",
    "    file_list = os.listdir(f'./{path}/token')\n",
    "    csv_list = []\n",
    "    for file in file_list:\n",
    "        if os.path.splitext(file)[1] == '.csv':\n",
    "            csv_list.append(file)\n",
    "\n",
    "    merge_file_values = list(itertools.chain(*merge_file.values()))\n",
    "\n",
    "    #결합대상의 파일명을 가져오거나, 대상이 아닌 파일을 필터링\n",
    "    files = dict()\n",
    "    for file in csv_list:\n",
    "        keyword = file.split('_')[0].split(\"+\")[0].replace(' ', '')\n",
    "        if keyword in merge_file_values:\n",
    "            for key in merge_file.keys():\n",
    "                if keyword in merge_file[key]:\n",
    "                    if files.get(key) == None:\n",
    "                        files[key] = [file]\n",
    "                    else:\n",
    "                        files[key].append(file)\n",
    "        else :\n",
    "            #필터링\n",
    "            file_df = pd.read_csv(f'./{path}/token/{file}')\n",
    "            file_df = file_df.fillna('')\n",
    "            filtered = file_df['full_text'].apply(lambda x : any(ele in x for ele in filter_word)) #전체글\n",
    "            clean_file = file_df[~filtered]\n",
    "            filtered = clean_file.title.apply(lambda x : any(ele in x for ele in filter_word)) #타이틀\n",
    "            clean_file = clean_file[~filtered]\n",
    "            \n",
    "            total_len = clean_file.shape[0]\n",
    "            clean_file.to_csv(f'./{path}/token_통합/{keyword}_{total_len}.csv',index = False)\n",
    "            \n",
    "    #파일결합\n",
    "    for key in tqdm_notebook(files.keys()):\n",
    "        merged_df = pd.DataFrame()\n",
    "        for file in files[key]:\n",
    "            file_df = pd.read_csv(f'./{path}/token/{file}')\n",
    "            merged_df = pd.concat([merged_df,file_df])\n",
    "        merged_df = merged_df.drop_duplicates('full_text')\n",
    "\n",
    "        #필터링\n",
    "        merged_df = merged_df.fillna('')\n",
    "        filtered = merged_df['full_text'].apply(lambda x : any(ele in x for ele in filter_word)) #전체글\n",
    "        clean_file = merged_df[~filtered]\n",
    "        filtered = clean_file.title.apply(lambda x : any(ele in x for ele in filter_word)) #타이틀\n",
    "        clean_file = clean_file[~filtered]\n",
    "\n",
    "        total_len = clean_file.shape[0]\n",
    "        clean_file.to_csv(f'./{path}/token_통합/{key}_{total_len}.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T09:01:54.783393Z",
     "start_time": "2021-01-13T09:01:24.354481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb624232010a48f5b8818875c7a39c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f84a23e12c40ff853d47812513e7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "separate_merge(path)\n",
    "setting_file(path, merge_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일 종합 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T09:03:14.860017Z",
     "start_time": "2021-01-13T09:03:14.857017Z"
    }
   },
   "outputs": [],
   "source": [
    "path = 'new_output'\n",
    "def result_df(path):\n",
    "    file_list = os.listdir(f'./{path}/token_통합/')\n",
    "\n",
    "    total_df = pd.DataFrame()\n",
    "    for file in file_list:\n",
    "        sumup = dict()\n",
    "        keyword = file.split('_')[0]\n",
    "        df = pd.read_csv(f'./{path}/token_통합/{file}')\n",
    "        sumup['total_size'] = df.shape[0]\n",
    "        for year in range(2010,2020):\n",
    "            try :\n",
    "                sumup[year] = df[df.year == year].shape[0]\n",
    "            except :\n",
    "                sumup[year] = np.nan\n",
    "        one_df = pd.DataFrame(sumup,index =[keyword])\n",
    "        total_df = pd.concat([total_df,one_df])\n",
    "    return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
