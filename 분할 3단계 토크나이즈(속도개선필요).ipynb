{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# issue\n",
    "* tokenize 작업의 속도가 굉장히 느림 개선방법이 있다면 고민해봐야함 (작업내용 1차적으로 보존)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T07:48:55.939632Z",
     "start_time": "2020-05-08T07:48:52.076472Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:217: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook  # 진행과정 시각화\n",
    "from datetime import timedelta  # 시간날짜\n",
    "from apyori import apriori  # 연관분석\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup  # 크롤링\n",
    "\n",
    "import wordcloud\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Komoran,Okt,Kkma,Twitter  # 자연어처리\n",
    "# from eunjeon import Mecab #은전한닢\n",
    "okt = Okt()\n",
    "# mecab = Mecab()\n",
    "komoran = Komoran(userdic='user_dictionary.txt')\n",
    "\n",
    "#한글깨짐방지\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<style>.container {width:100% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T07:51:31.897202Z",
     "start_time": "2020-05-08T07:51:31.892201Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tokenized(tagger, dataframe, keyword, stopword):\n",
    "    '''\n",
    "    tagger : 형태소분석기 무엇사용?\n",
    "    dataframe : 분석할 df\n",
    "    keyword : sw에 추가하기 위함\n",
    "    '''\n",
    "    #저장할 위치\n",
    "    tokens_dict = dict()\n",
    "    tokens_dict['Nouns'] = []\n",
    "    tokens_dict['Adj'] = []\n",
    "\n",
    "    # stopword에 검색어 추가\n",
    "    sw.append(keyword.split(' ')[0])\n",
    "    \n",
    "    num = 1 #중간저장용\n",
    "    count = 1 #구분기호\n",
    "        \n",
    "    for txt in tqdm_notebook(dataframe['full_text'] ,desc = keyword+'tokenize'):\n",
    "        tagged = tagger.pos(txt, norm=True, stem=True)\n",
    "\n",
    "        tagged_df = pd.DataFrame(tagged)\n",
    "        noun_lists = tagged_df[tagged_df[1].isin(['Noun'])][0].values\n",
    "        adj_lists = tagged_df[tagged_df[1].isin(['Adjective'])][0].values\n",
    "        #클리닝\n",
    "        clean_nouns = list(filter(lambda word : (len(word)>1)&(word not in sw),noun_lists))\n",
    "        clean_adjs = list(filter(lambda word : (len(word)>1)&(word not in sw),adj_lists))\n",
    "        #dictionay 저장\n",
    "        tokens_dict.get('Nouns',0).append(clean_nouns)\n",
    "        tokens_dict.get('Adj',0).append(clean_adjs)\n",
    "        gc.collect()\n",
    "    \n",
    "        num += 1\n",
    "    \n",
    "        if num == 100:\n",
    "            #중간저장 \n",
    "            tokens_df = pd.DataFrame(tokens_dict)\n",
    "            tokens_df.to_csv('D:/Python/블로그크롤링/output/token/{keyword}_token_{count}.csv'.format(keyword = keyword,count = count)) #token을 원래문장과 비교하려면 동일한 인덱스로 추정하면됨\n",
    "            count += 1\n",
    "            #초기화 \n",
    "            num = 1\n",
    "            tokens_dict = dict()\n",
    "            tokens_dict['Nouns'] = []\n",
    "            tokens_dict['Adj'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T09:06:37.133820Z",
     "start_time": "2020-05-08T07:51:32.735391Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94efbb81521a4584a60cd4d9120d471a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='강릉tokenize', max=81699.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "java.lang.OutOfMemoryError",
     "evalue": "java.lang.OutOfMemoryError: Java heap space",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36morg.openkoreantext.processor.tokenizer.KoreanTokenizer$$$Lambda$114/0x0000000800cd6040.apply\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36morg.openkoreantext.processor.tokenizer.KoreanTokenizer$.$anonfun$tokenizeTopN$1\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36morg.openkoreantext.processor.tokenizer.KoreanTokenizer$.parseKoreanChunk\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36morg.openkoreantext.processor.tokenizer.KoreanTokenizer$.findTopCandidates\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.immutable.Range.foreach$mVc$sp\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36morg.openkoreantext.processor.tokenizer.KoreanTokenizer$$$Lambda$117/0x0000000800ce0840.apply$mcVI$sp\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36morg.openkoreantext.processor.tokenizer.KoreanTokenizer$.$anonfun$findTopCandidates$1\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.immutable.Range.foreach\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36morg.openkoreantext.processor.tokenizer.KoreanTokenizer$$$Lambda$119/0x0000000800ce2040.apply\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36morg.openkoreantext.processor.tokenizer.KoreanTokenizer$.$anonfun$findTopCandidates$2$adapted\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36morg.openkoreantext.processor.tokenizer.KoreanTokenizer$.$anonfun$findTopCandidates$2\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.AbstractSeq.sortBy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.SeqLike.sortBy$\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.SeqLike.sortBy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.AbstractSeq.sorted\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.SeqLike.sorted$\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.SeqLike.sorted\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mjava.util.Arrays.sort\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mjava.util.TimSort.sort\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mjava.util.TimSort.binarySort\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.math.Ordering$$anon$5.compare\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36morg.openkoreantext.processor.tokenizer.KoreanTokenizer$$$Lambda$128/0x0000000800cfa840.apply\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36morg.openkoreantext.processor.tokenizer.KoreanTokenizer$.$anonfun$findTopCandidates$10\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36morg.openkoreantext.processor.tokenizer.ParsedChunk.posTieBreaker\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.immutable.List.map\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.TraversableLike.map$\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.TraversableLike.map\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.immutable.List.foreach\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.TraversableLike$$Lambda$38/0x0000000800c2d040.apply\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.TraversableLike.$anonfun$map$1\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.mutable.ListBuffer.$plus$eq\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\_jpype.cp36-win_amd64.pyd\u001b[0m in \u001b[0;36mscala.collection.mutable.ListBuffer.$plus$eq\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Java Exception",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mjava.lang.OutOfMemoryError\u001b[0m                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ced0d08ecd99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mkeyword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtokenized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mokt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmain_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-51c6e2fa0bf9>\u001b[0m in \u001b[0;36mtokenized\u001b[1;34m(tagger, dataframe, keyword, stopword)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'tokenize'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mtagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtagged_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\konlpy\\tag\\_okt.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBoolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                     jpype.java.lang.Boolean(stem)).toArray()\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mjava.lang.OutOfMemoryError\u001b[0m: java.lang.OutOfMemoryError: Java heap space"
     ]
    }
   ],
   "source": [
    "sw = list(pd.read_csv(\"stopword.csv\")['불용어']) #불용어 불러오기\n",
    "path = 'D:/Python/블로그크롤링/output/크롤링/통합/'\n",
    "file_list = os.listdir(path)\n",
    "for file in file_list:\n",
    "    main_df = pd.read_csv(path + file)\n",
    "    keyword = file_list[0].split('_')[0]\n",
    "    tokenized(okt,main_df,keyword,sw)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T07:48:56.096079Z",
     "start_time": "2020-05-08T07:48:55.985642Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fa574757fe35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtokens_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Nouns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtokens_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Adj'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mblog_num\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'tokenize'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#블로그 품사태깅 (load한 경우 list가 아닌 text로 받아서 첫번째가 아닌 전체를 받는다)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpos_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mokt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblog_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'main_df' is not defined"
     ]
    }
   ],
   "source": [
    "#저장할 위치\n",
    "tokens_dict = dict()\n",
    "tokens_dict['Nouns'] = []\n",
    "tokens_dict['Adj'] = []\n",
    "for blog_num in tqdm_notebook(range(0,len(main_df[:500])),desc = keyword+'tokenize'):\n",
    "#블로그 품사태깅 (load한 경우 list가 아닌 text로 받아서 첫번째가 아닌 전체를 받는다)\n",
    "    pos_text = okt.pos(main_df['full_text'][blog_num], norm=True, stem=True)\n",
    "\n",
    "    pos_text_df = pd.DataFrame(pos_text)\n",
    "    noun_lists = pos_text_df[pos_text_df[1].apply(lambda x : (x in \"Noun\"))][0].values\n",
    "    adj_lists = pos_text_df[pos_text_df[1].apply(lambda x : (x in \"Adjective\"))][0].values\n",
    "    #클리닝\n",
    "    clean_nouns = list(filter(lambda word : (len(word)>1)&(word not in sw),noun_lists))\n",
    "    clean_adjs = list(filter(lambda word : (len(word)>1)&(word not in sw),adj_lists))\n",
    "    #dictionay 저장\n",
    "    tokens_dict.get('Nouns',0).append(clean_nouns)\n",
    "    tokens_dict.get('Adj',0).append(clean_adjs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 속도가 느려서 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T07:17:56.772622Z",
     "start_time": "2020-05-08T07:16:43.600351Z"
    }
   },
   "outputs": [],
   "source": [
    "#ori \n",
    "#저장할 위치\n",
    "tokens_dict = dict()\n",
    "tokens_dict['Nouns'] = []\n",
    "tokens_dict['Adj'] = []\n",
    "for blog_num in tqdm_notebook(range(0,len(main_df[:500])),desc = keyword+'tokenize'):\n",
    "    #블로그 품사태깅 (load한 경우 list가 아닌 text로 받아서 첫번째가 아닌 전체를 받는다)\n",
    "#     pos_text = okt.pos(target[blog_num][0], norm=True, stem=True)\n",
    "        pos_text = okt.pos(main_df['full_text'][blog_num], norm=True, stem=True)\n",
    "\n",
    "        pos_text_df = pd.DataFrame(pos_text)\n",
    "        noun_lists = pos_text_df[pos_text_df[1].apply(lambda x : (x in \"Noun\"))][0].values\n",
    "        adj_lists = pos_text_df[pos_text_df[1].apply(lambda x : (x in \"Adjective\"))][0].values\n",
    "        #클리닝\n",
    "        clean_nouns = list(filter(lambda word : (len(word)>1)&(word not in sw),noun_lists))\n",
    "        clean_adjs = list(filter(lambda word : (len(word)>1)&(word not in sw),adj_lists))\n",
    "        #dictionay 저장\n",
    "        tokens_dict.get('Nouns',0).append(clean_nouns)\n",
    "        tokens_dict.get('Adj',0).append(clean_adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T06:36:38.665151Z",
     "start_time": "2020-05-08T06:35:31.552645Z"
    }
   },
   "outputs": [],
   "source": [
    "#tunning 1\n",
    "#저장할 위치\n",
    "tokens_dict = dict()\n",
    "tokens_dict['Nouns'] = []\n",
    "tokens_dict['Adj'] = []\n",
    "\n",
    "for txt in tqdm_notebook(main_df['full_text'].values[:500] ,desc = keyword+'tokenize'):\n",
    "    a = okt.pos(txt, norm=True, stem=True)\n",
    "    noun_lists = []\n",
    "    adj_lists = []\n",
    "    for word, dtype in a:\n",
    "        if dtype == 'Noun':\n",
    "            noun_lists.append(word)\n",
    "        elif dtype == 'Adjective':\n",
    "            adj_lists.append(word)\n",
    "    #클리닝\n",
    "    clean_nouns = list(filter(lambda word : (len(word)>1)&(word not in sw),noun_lists))\n",
    "    clean_adjs = list(filter(lambda word : (len(word)>1)&(word not in sw),adj_lists))\n",
    "    #dictionay 저장\n",
    "    tokens_dict.get('Nouns',0).append(clean_nouns)\n",
    "    tokens_dict.get('Adj',0).append(clean_adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T07:46:59.799434Z",
     "start_time": "2020-05-08T07:45:52.566395Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tunning 2 : DF.isin\n",
    "#저장할 위치\n",
    "tokens_dict = dict()\n",
    "tokens_dict['Nouns'] = []\n",
    "tokens_dict['Adj'] = []\n",
    "num = 1\n",
    "count = 1\n",
    "for txt in tqdm_notebook(main_df['full_text'].values[:500] ,desc = keyword+'tokenize'):\n",
    "    \n",
    "    a = okt.pos(txt, norm=True, stem=True)\n",
    "\n",
    "    a_df = pd.DataFrame(a)\n",
    "    noun_lists = a_df[a_df[1].isin(['Noun'])][0].values\n",
    "    adj_lists = a_df[a_df[1].isin(['Adjective'])][0].values\n",
    "    #클리닝\n",
    "    clean_nouns = list(filter(lambda word : (len(word)>1)&(word not in sw),noun_lists))\n",
    "    clean_adjs = list(filter(lambda word : (len(word)>1)&(word not in sw),adj_lists))\n",
    "    #dictionay 저장\n",
    "    tokens_dict.get('Nouns',0).append(clean_nouns)\n",
    "    tokens_dict.get('Adj',0).append(clean_adjs)\n",
    "    \n",
    "    gc.collect()\n",
    "    num += 1\n",
    "    \n",
    "    if num == 100:\n",
    "        #중간저장 \n",
    "        tokens_df = pd.DataFrame(tokens_dict)\n",
    "        tokens_df.to_csv('D:/Python/블로그크롤링/output/token/{keyword}_token_{count}.csv'.format(keyword = keyword,count = count)) #token을 원래문장과 비교하려면 동일한 인덱스로 추정하면됨\n",
    "        count += 1\n",
    "        num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T07:14:50.360322Z",
     "start_time": "2020-05-08T07:13:40.043994Z"
    }
   },
   "outputs": [],
   "source": [
    "#tunning 3 : np.isin\n",
    "#저장할 위치\n",
    "tokens_dict = dict()\n",
    "tokens_dict['Nouns'] = []\n",
    "tokens_dict['Adj'] = []\n",
    "#append 미리선언\n",
    "add_noun = tokens_dict.get('Nouns',0).append\n",
    "add_adj = tokens_dict.get('Adj',0).append\n",
    "for txt in tqdm_notebook(main_df['full_text'].values[:500] ,desc = keyword+'tokenize'):\n",
    "    tagged = okt.pos(txt, norm=True, stem=True)\n",
    "    \n",
    "    #array로 변환후 작업\n",
    "    tagged_arr= np.array(tagged)\n",
    "\n",
    "#     n_mask = np.isin(tagged_arr[:,1],\"Noun\")\n",
    "#     a_mask = np.isin(tagged_arr[:,1],\"Adjective\")\n",
    "\n",
    "    noun_lists = tagged_arr[np.isin(tagged_arr[:,1],\"Noun\")][:,0]\n",
    "    adj_lists = tagged_arr[np.isin(tagged_arr[:,1],\"Adjective\")][:,0]\n",
    "    #클리닝\n",
    "    clean_nouns = list(filter(lambda word : (len(word)>1)&(word not in sw),noun_lists))\n",
    "    clean_adjs = list(filter(lambda word : (len(word)>1)&(word not in sw),adj_lists))\n",
    "    #dictionay 저장\n",
    "    add_noun(clean_nouns)\n",
    "    add_adj(clean_adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
