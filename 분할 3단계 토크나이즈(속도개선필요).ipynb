{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# issue\n",
    "* tokenize 작업의 속도가 굉장히 느림 개선방법이 있다면 고민해봐야함 (작업내용 1차적으로 보존)\n",
    "* 2만개까지는 heap meomory 에러가 발생하지 않았음 따라서 2만개 이상일때는 분할작업진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T04:14:24.451550Z",
     "start_time": "2020-06-05T04:14:20.554672Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:217: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm,tqdm_notebook  # 진행과정 시각화\n",
    "tqdm.pandas() #apply사용\n",
    "from datetime import timedelta  # 시간날짜\n",
    "from apyori import apriori  # 연관분석\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup  # 크롤링\n",
    "\n",
    "import wordcloud\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Komoran,Okt,Kkma,Twitter  # 자연어처리\n",
    "# from eunjeon import Mecab #은전한닢\n",
    "okt = Okt(max_heap_size=4096)\n",
    "# mecab = Mecab()\n",
    "komoran = Komoran(userdic='user_dictionary.txt')\n",
    "\n",
    "#한글깨짐방지\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<style>.container {width:100% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T04:14:24.461552Z",
     "start_time": "2020-06-05T04:14:24.452550Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tokenized(dataframe, file_name, keyword, stopword):\n",
    "    '''\n",
    "    okt : 형태소분석기 무엇사용?\n",
    "    dataframe : 분석할 df\n",
    "    keyword : sw에 추가하기 위함\n",
    "    '''\n",
    "\n",
    "    # stopword에 검색어 추가\n",
    "    stopword.append(keyword.split(' ')[0])\n",
    "    \n",
    "    start = 0\n",
    "    end = len(dataframe)\n",
    "    \n",
    "    print(keyword)\n",
    "    if end<=20000:    \n",
    "        # tqdm.pandas(desc = '토큰화')\n",
    "        token_df = dataframe.iloc[start:end]['full_text'].progress_apply(lambda x: okt.pos(x, norm=True, stem=True))\n",
    "        token_df_e = token_df.explode()\n",
    "        # tqdm.pandas(desc = '단어추출')\n",
    "        token_noun = token_df_e.apply(lambda x: x[0] if x[1] == 'Noun' else np.nan).dropna()\n",
    "        token_adj = token_df_e.apply(lambda x: x[0] if x[1] == 'Adjective' else np.nan).dropna()\n",
    "\n",
    "        # 클리닝\n",
    "        # tqdm.pandas(desc = '클리닝')\n",
    "        token_noun = token_noun[token_noun.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "        token_adj = token_adj[token_adj.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "        \n",
    "        # 저장\n",
    "#         defalut = pd.Series(index=range(start, end), data=range(start, end), name='index')\n",
    "        dataframe = pd.merge(dataframe, token_noun.groupby(level=0).agg(list).rename(\n",
    "            'Noun'), how='left', left_index=True, right_index=True)\n",
    "        dataframe = pd.merge(dataframe, token_adj.groupby(level=0).agg(list).rename(\n",
    "            'Adjective'), how='left', left_index=True, right_index=True)\n",
    "        pd.DataFrame(dataframe).to_csv(f'./output/token/{keyword}_{start}~{end}.csv')\n",
    "        \n",
    "    else : #20000개 이상인경우 분할 작업\n",
    "        final_end = len(dataframe)\n",
    "        epoch = (final_end//20000) +1\n",
    "        start = 0\n",
    "        end = 20000\n",
    "        while epoch != 0:\n",
    "            # tqdm.pandas(desc = '토큰화')\n",
    "            token_df = dataframe.iloc[start:end]['full_text'].progress_apply(lambda x: okt.pos(x, norm=True, stem=True))\n",
    "            token_df_e = token_df.explode()\n",
    "            # tqdm.pandas(desc = '단어추출')\n",
    "            token_noun = token_df_e.apply(lambda x: x[0] if x[1] == 'Noun' else np.nan).dropna()\n",
    "            token_adj = token_df_e.apply(lambda x: x[0] if x[1] == 'Adjective' else np.nan).dropna()\n",
    "\n",
    "            # 클리닝\n",
    "            # tqdm.pandas(desc = '클리닝')\n",
    "            token_noun = token_noun[token_noun.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "            token_adj = token_adj[token_adj.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "\n",
    "            #저장\n",
    "            # defalut = pd.Series(index=range(start, end), data=range(start, end), name='index')\n",
    "            dataframe = pd.merge(dataframe, token_noun.groupby(level=0).agg(list).rename(\n",
    "                'Noun'), how='left', left_index=True, right_index=True)\n",
    "            dataframe = pd.merge(dataframe, token_adj.groupby(level=0).agg(list).rename(\n",
    "                'Adjective'), how='left', left_index=True, right_index=True)\n",
    "            \n",
    "            os.makedirs(\n",
    "            './output/token/{file_name}'.format(file_name=file_name), exist_ok=True)\n",
    "            pd.DataFrame(dataframe).to_csv(f'./output/token/{file_name}/{file_name}_{start}~{end}.csv')\n",
    "            \n",
    "            #next turn\n",
    "            if epoch >2 :\n",
    "                start += 20000\n",
    "                end += 20000\n",
    "            else :      \n",
    "                start += 20000\n",
    "                end = final_end\n",
    "\n",
    "            tokens_dict = dict()\n",
    "            tokens_dict['Nouns'] = []\n",
    "            tokens_dict['Adj'] = []\n",
    "            epoch -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### heap 메모리 부족 오류 발생(해결법 파악못함) -> 2만개 이하에서 제대로 작동하는것으로 보임 2020-06-01 17:22:38 \n",
    "-> 추가로 중간에 누락되는 index가 존재하며, 그 인덱스를 파악하기 어려움(인덱스 번호 보존을 안해두기때문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-05T04:14:20.543Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강원도\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 20000/20000 [17:30<00:00, 19.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 20000/20000 [21:12<00:00, 15.72it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 20000/20000 [23:02<00:00, 14.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 20000/20000 [24:00<00:00, 13.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5670/5670 [06:46<00:00, 13.95it/s]\n",
      "  0%|                                                                                 | 2/8739 [00:00<07:30, 19.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경포해수욕장 +강원도\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8739/8739 [18:53<00:00,  7.71it/s]\n",
      "  0%|                                                                                 | 3/7973 [00:00<05:05, 26.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "광진 +강원도\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████▉                                                                | 1424/7973 [07:15<18:32,  5.89it/s]"
     ]
    }
   ],
   "source": [
    "#임시 속초/속초해수욕장\n",
    "sw = list(pd.read_excel(\"stopword(cp949).xlsx\",encoding = 'cp949')['불용어']) #불용어 불러오기\n",
    "path = 'D:/Python/블로그크롤링/output/크롤링/통합/'\n",
    "file_list = os.listdir(path)\n",
    "for file in file_list[1:]:\n",
    "    file_name = file.split('.')[0]\n",
    "    main_df = pd.read_csv(path + file)\n",
    "    keyword = file.split('_')[0]\n",
    "    tokenized(main_df,file_name,keyword,sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-05T04:14:20.544Z"
    }
   },
   "outputs": [],
   "source": [
    "# file = '광진 +양양_통합_5044.csv'\n",
    "# file_name = file.split('.')[0]\n",
    "# main_df = pd.read_csv(path + file)\n",
    "# keyword = file.split('_')[0]\n",
    "# tokenized(main_df,file_name,keyword,sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-05T04:14:20.545Z"
    }
   },
   "outputs": [],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
