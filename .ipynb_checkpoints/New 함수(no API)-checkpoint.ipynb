{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목적\n",
    "* 블로그 내용을 긁어와서 연관분석 진행\n",
    "1. keyword관련 블로그 크롤링 \n",
    "    정확도 기준으로 긁어오되 날짜도 같이 가져와서 나중 어떤 날짜에 집중되어 있는지 확인\n",
    "    한 블로그당 단어빈도수 분석(필요할까)\n",
    "2. 블로그 내 단어 토크나이즈하고 \n",
    "3. 연관분석실시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:07:06.903494Z",
     "start_time": "2020-03-31T13:07:02.287226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apyori\n",
      "  Downloading apyori-1.1.2.tar.gz (8.6 kB)\n",
      "Building wheels for collected packages: apyori\n",
      "  Building wheel for apyori (setup.py): started\n",
      "  Building wheel for apyori (setup.py): finished with status 'done'\n",
      "  Created wheel for apyori: filename=apyori-1.1.2-py3-none-any.whl size=5984 sha256=5235bb621f4450c8a0ed214694dd194b7f3b0f6a4cf1cb568b9531188bd48e80\n",
      "  Stored in directory: c:\\users\\try00\\appdata\\local\\pip\\cache\\wheels\\47\\6f\\0f\\21a86f3679f7ed6bbe4dc6694f86818c5d85c2044bfab0f1e8\n",
      "Successfully built apyori\n",
      "Installing collected packages: apyori\n",
      "Successfully installed apyori-1.1.2\n"
     ]
    }
   ],
   "source": [
    "# # 선행설치\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install requests\n",
    "# !pip install lxm\n",
    "# # 연관분석\n",
    "# !pip install apyori\n",
    "# # 자연어처리\n",
    "# !pip install konlpy (JDK 설치가 되어있어야함 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T00:38:22.340848Z",
     "start_time": "2020-04-02T00:38:19.481806Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook  # 진행과정 시각화\n",
    "from datetime import timedelta  # 시간날짜\n",
    "from apyori import apriori  # 연관분석\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup  # 크롤링\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Komoran  # 자연어처리\n",
    "komoran = Komoran(userdic='./user_dictionary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:14:11.318372Z",
     "start_time": "2020-04-01T13:14:11.316371Z"
    }
   },
   "outputs": [],
   "source": [
    "#페이지 카운트 수로 api호출된다\n",
    "#1,000개일때 3분 \n",
    "#100*100개 만개의 경우 \"비정상적인 트래픽을 감지\"해서 오류걸림 \n",
    "naver_client_id = \"cGDVdveeg4egAgRCvXtA\"\n",
    "naver_client_secret = \"pFnYuTsat0\"\n",
    "keyword = '양양'\n",
    "display_count = 1\n",
    "page_count = 1000\n",
    "sort_type = 'sim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:14:15.344308Z",
     "start_time": "2020-04-01T13:14:15.340308Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_outword(string):\n",
    "    #이모지제거\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    #분석에 어긋나는 불용어구 제외 (특수문자, 의성어)\n",
    "    han = re.compile(r'[ㄱ-ㅎㅏ-ㅣ!?~,\".\\n\\r#\\ufeff\\u200d\\u200b]')\n",
    "    \n",
    "    string = emoji_pattern.sub(r'',string)\n",
    "    string = han.sub(r'',string)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:41:58.069642Z",
     "start_time": "2020-04-01T15:41:58.065641Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def time_change(x):\n",
    "    '''\n",
    "    검색결과에 있는 문자를 날짜로 변경\n",
    "    '''\n",
    "    min_pattern = re.compile('[0-9]+'+\"분 전\")\n",
    "    hour_pattern = re.compile('[0-9]+'+\"시간 전\")\n",
    "    day_pattern = re.compile('[0-9]+'+\"일 전\")\n",
    "\n",
    "    today = datetime.datetime.today().date()\n",
    "    # 일자\n",
    "    try:\n",
    "        d = re.findall(day_pattern, x)[0][0]\n",
    "        x = today - timedelta(days=int(d))\n",
    "    except:\n",
    "        pass\n",
    "    # 시간\n",
    "    try:\n",
    "        d = re.findall(hour_pattern, x)[0]\n",
    "        x = today\n",
    "    except:\n",
    "        pass\n",
    "    # 분\n",
    "    try:\n",
    "        h = re.findall(min_pattern, x)[0]\n",
    "        x = today\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if x == '어제':\n",
    "        x = today - timedelta(days=1)\n",
    "\n",
    "    elif type(x) == str:\n",
    "        x = datetime.datetime.strptime(x.replace('.', '-')[:-1], '%Y-%m-%d')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T00:55:26.783964Z",
     "start_time": "2020-04-01T00:55:26.780971Z"
    }
   },
   "outputs": [],
   "source": [
    "# #네이버에서 검색했을때 url구조\n",
    "# https://search.naver.com/search.naver\n",
    "#     ?date_from=&\n",
    "#     date_option=0&\n",
    "#     date_to=&\n",
    "#     dup_remove=1&\n",
    "#     nso=&\n",
    "#     post_blogurl=\n",
    "#     &post_blogurl_without=\n",
    "#     &query=%EC%96%91%EC%96%91\n",
    "#     &sm=tab_pge\n",
    "#     &srchby=all\n",
    "#     &st=sim\n",
    "#     &where=post\n",
    "#     &start=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample 상세옵션없음\n",
    "https://search.naver.com/search.naver?date_from=&date_option=0&date_to=&dup_remove=1&nso=&post_blogurl=&post_blogurl_without=&query=%EC%96%91%EC%96%91&sm=tab_pge&srchby=all&st=sim&where=post&start=1\n",
    "https://search.naver.com/search.naver?\n",
    "    date_from=&\n",
    "    date_option=0&\n",
    "    date_to=&\n",
    "    dup_remove=1&\n",
    "    nso=&\n",
    "    post_blogurl=&\n",
    "    post_blogurl_without=&\n",
    "    query=%EC%96%91%EC%96%91&\n",
    "    sm=tab_pge&\n",
    "    srchby=all&\n",
    "    st=sim&\n",
    "    where=post&\n",
    "    start=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "date_from - YYYYMMDD : 시작날짜 date_option 8일때   \n",
    "date_option - int : 0 : 전체, 2 : 1일, 3 : 1주, 4 : 1개월, 6 : 6개월, 7 : 1년, 8 : 기간지정  \n",
    "date_to - YYYYMMDD : 마지막날짜 date_option 8일때  \n",
    "dup_remove - int : 유사문서제거옵션 1 : 제거, 0 : 유지  \n",
    "nso - str : _상세검색으로 추정_   \n",
    "post_blogurl - ?  \n",
    "post_blogurl_without - ?   \n",
    "query - str : 검색어     \n",
    "sm=tab_pge -?    \n",
    "st - str : 정렬순서 sim : 유사도 date : 날짜    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T09:10:04.282001Z",
     "start_time": "2020-04-01T09:10:04.279009Z"
    }
   },
   "outputs": [],
   "source": [
    "keyword = \"양양\"\n",
    "start_num = 1\n",
    "end_num = 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T08:39:49.323600Z",
     "start_time": "2020-04-01T08:39:49.314599Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#날짜수정가능하게 하는거 에러남\n",
    "def naver_blog_crawling(keyword, start_num=1, end_num=101,date_option=0,date_from='',date_to=''):\n",
    "    '''\n",
    "    네이버 블로그 크롤링 함수\n",
    "    네이버 블로그 검색결과를 크롤링하며, 1페이지당 10개씩을 검색한다\n",
    "    \n",
    "    keyword : string\n",
    "     검색하고 싶은 키워드를 넣는다\n",
    "    start_num : int (default = 1) \n",
    "     시작할 위치 1로 끝나는 단위 추천\n",
    "    end_num : int (default = 101)\n",
    "     끝나는 위치 1로 끝나는 단위 추천\n",
    "    date_option : int (default = 0)\n",
    "     주어지는 숫자에 의해 검색방법이 변경됨\n",
    "     0 : 전체, 2 : 1일, 3 : 1주, 4 : 1개월, 6 : 6개월, 7 : 1년, 8 : 기간지정\n",
    "    date_from : YYYYMMDD (default = \"\")\n",
    "     date_option이 8일때 사용 검색 시작일자를 지정\n",
    "    date_to : YYYMMDD (default = \"\")\n",
    "     date_option이 8일때 사용 검색 마지막일자를 지정\n",
    "    '''\n",
    "    #url 찾는 패턴\n",
    "    pattern =re.compile('href=\"'+'[A-z0-9\\:\\/\\&\\;\\.\\?\\=]+')\n",
    "\n",
    "    #저장위치\n",
    "    postdates = []\n",
    "    strings = []\n",
    "    urls = []\n",
    "    titles = []\n",
    "    output_error = []\n",
    "    connection_error = []\n",
    "    count = 1\n",
    "\n",
    "    #keyword와 시작넘버만 바꾸면서 진행하게끔\n",
    "    base_url = 'https://search.naver.com/search.naver?date_from={date_from}&date_option={date_option}&date_to={date_to}&dup_remove=1&nso=&post_blogurl=&post_blogurl_without=&query={keyword}&sm=tab_pge&srchby=all&st=sim&where=post&start={start}'\n",
    "\n",
    "    #for문 돌려야하는 부분(각 검색결과의 시작은 1이고, 10개씩 보여짐,)\n",
    "    for i in tqdm_notebook(range(start_num,end_num+1,10),desc = \"page work\"):\n",
    "        search_list = base_url.format(keyword = keyword,start = i,\n",
    "                                      date_option = date_option, date_from = date_from, date_to=date_to)\n",
    "        response = requests.get(search_list)\n",
    "\n",
    "        if response.status_code is 200:\n",
    "            #1번 검색시 10개의 결과가 출력 따라서 section은 총 10개\n",
    "            soup = BeautifulSoup(response.content,'lxml')\n",
    "            sections = soup.findAll('li', attrs={'class':'sh_blog_top'})\n",
    "\n",
    "            for section in sections:\n",
    "                try:\n",
    "                    #href부분만 가져오기 어려워서 정규표현식으로 검색\n",
    "                    url = re.findall(pattern, str(section))[0].replace('?Redirect=Log&amp;logNo=','/').replace('href=\"','')\n",
    "                    title = section.select_one('a.txt84').text\n",
    "                    date = section.select_one('dd.txt_inline').text.strip()\n",
    "\n",
    "                    #블로그 url안에 들어가기(아직 크롤링불가)\n",
    "                    get_blog_post_content_code = requests.get(url)\n",
    "                    get_blog_post_content_text = get_blog_post_content_code.text\n",
    "                    get_blog_post_content_soup = BeautifulSoup(get_blog_post_content_text, 'lxml')\n",
    "\n",
    "                    #크롤링가능한 url에 접속\n",
    "                    real_blog_post_url = \"http://blog.naver.com\" + get_blog_post_content_soup.select('#mainFrame')[0].get('src')\n",
    "                    get_real_blog_post_content_code = requests.get(real_blog_post_url)\n",
    "                    get_real_blog_post_content_text = get_real_blog_post_content_code.text\n",
    "                    get_real_blog_post_content_soup = BeautifulSoup(get_real_blog_post_content_text, 'lxml')\n",
    "\n",
    "                    # url (에러나면 위에서부터 에러남)\n",
    "                    urls.append(real_blog_post_url)\n",
    "                    #블로그명\n",
    "                    titles.append(title)\n",
    "                    #날짜\n",
    "                    postdates.append(date)\n",
    "\n",
    "                    #본문  (postviewarea 패턴과 se-main-container 2가지 유형이 있어 분리함) \n",
    "                    try:\n",
    "                        blog_post_content = get_real_blog_post_content_soup.select('div#postViewArea')\n",
    "                        if len(blog_post_content) == 0:\n",
    "                            blog_post_content = get_real_blog_post_content_soup.select('div.se-main-container')\n",
    "\n",
    "                        string = \"\"\n",
    "                        for sentence in blog_post_content[0].stripped_strings:\n",
    "                            string += \" \"+sentence.replace('\\xa0',\" \")\n",
    "                        strings.append([string])\n",
    "                        count += 1\n",
    "                    except:\n",
    "                        strings.append([np.NaN])\n",
    "                        count += 1\n",
    "\n",
    "                except Exception as ex: \n",
    "                    print('가져오기에러 {num}번째'.format(num = count),ex)\n",
    "                    output_error.append(count)\n",
    "                    count += 1\n",
    "                    pass\n",
    "            else:\n",
    "                print('연결오류 {num}번째'.format(num = count),response.status_code)\n",
    "                connection_error.append(count)\n",
    "                count += 1\n",
    "                \n",
    "    crawling_df = pd.DataFrame({\"post_dates\":postdates, \"title\":titles, \"full_text\":strings, \"url\":urls})\n",
    "    return crawling_df,output_error,connection_error\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T00:30:35.453508Z",
     "start_time": "2020-04-02T00:30:35.443236Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def naver_blog_crawling(keyword, start_num=1, end_num=101):\n",
    "    '''\n",
    "    네이버 블로그 크롤링 함수\n",
    "    네이버 블로그 검색결과를 크롤링하며, 1페이지당 10개씩을 검색한다\n",
    "\n",
    "    keyword : string\n",
    "     검색하고 싶은 키워드를 넣는다\n",
    "    start_num : int (default = 1) \n",
    "     시작할 위치 1로 끝나는 단위 추천\n",
    "    end_num : int (default = 101)\n",
    "     끝나는 위치 1로 끝나는 단위 추천\n",
    "    date_option : int (default = 0)\n",
    "     주어지는 숫자에 의해 검색방법이 변경됨\n",
    "     0 : 전체, 2 : 1일, 3 : 1주, 4 : 1개월, 6 : 6개월, 7 : 1년, 8 : 기간지정\n",
    "    date_from : YYYYMMDD (default = \"\")\n",
    "     date_option이 8일때 사용 검색 시작일자를 지정\n",
    "    date_to : YYYMMDD (default = \"\")\n",
    "     date_option이 8일때 사용 검색 마지막일자를 지정\n",
    "    '''\n",
    "    # url 찾는 패턴\n",
    "    pattern = re.compile('href=\"'+'[A-z0-9\\:\\/\\&\\;\\.\\?\\=]+')\n",
    "\n",
    "    # 저장위치\n",
    "    postdates = []\n",
    "    strings = []\n",
    "    urls = []\n",
    "    titles = []\n",
    "    output_error = []\n",
    "    connection_error = []\n",
    "    count = 1\n",
    "\n",
    "    # keyword와 시작넘버만 바꾸면서 진행하게끔\n",
    "    base_url = 'https://search.naver.com/search.naver?date_from=&date_option=&date_to=&dup_remove=1&nso=&post_blogurl=&post_blogurl_without=&query={keyword}&sm=tab_pge&srchby=all&st=sim&where=post&start={start}'\n",
    "\n",
    "    # for문 돌려야하는 부분(각 검색결과의 시작은 1이고, 10개씩 보여짐,)\n",
    "    for i in tqdm_notebook(range(start_num, end_num+1, 10), desc=\"page work\"):\n",
    "        search_list = base_url.format(keyword=keyword, start=i)\n",
    "        response = requests.get(search_list)\n",
    "\n",
    "        if response.status_code is 200:\n",
    "            # 1번 검색시 10개의 결과가 출력 따라서 section은 총 10개\n",
    "            soup = BeautifulSoup(response.content, 'lxml')\n",
    "            sections = soup.findAll('li', attrs={'class': 'sh_blog_top'})\n",
    "\n",
    "            for section in sections:\n",
    "                try:\n",
    "                    # href부분만 가져오기 어려워서 정규표현식으로 검색\n",
    "                    url = re.findall(pattern, str(section))[0].replace(\n",
    "                        '?Redirect=Log&amp;logNo=', '/').replace('href=\"', '')\n",
    "                    title = section.select_one('a.txt84').text\n",
    "                    date = section.select_one('dd.txt_inline').text.strip()\n",
    "\n",
    "                    # 블로그 url안에 들어가기(아직 크롤링불가)\n",
    "                    get_blog_post_content_code = requests.get(url)\n",
    "                    get_blog_post_content_text = get_blog_post_content_code.text\n",
    "                    get_blog_post_content_soup = BeautifulSoup(\n",
    "                        get_blog_post_content_text, 'lxml')\n",
    "\n",
    "                    # 크롤링가능한 url에 접속\n",
    "                    real_blog_post_url = \"http://blog.naver.com\" + \\\n",
    "                        get_blog_post_content_soup.select('#mainFrame')[\n",
    "                            0].get('src')\n",
    "                    get_real_blog_post_content_code = requests.get(\n",
    "                        real_blog_post_url)\n",
    "                    get_real_blog_post_content_text = get_real_blog_post_content_code.text\n",
    "                    get_real_blog_post_content_soup = BeautifulSoup(\n",
    "                        get_real_blog_post_content_text, 'lxml')\n",
    "\n",
    "                    # url (에러나면 위에서부터 에러남)\n",
    "                    urls.append(real_blog_post_url)\n",
    "                    # 블로그명\n",
    "                    titles.append(title)\n",
    "                    # 날짜\n",
    "                    postdates.append(date)\n",
    "\n",
    "                    # 본문  (postviewarea 패턴과 se-main-container 2가지 유형이 있어 분리함)\n",
    "                    try:\n",
    "                        blog_post_content = get_real_blog_post_content_soup.select(\n",
    "                            'div#postViewArea')\n",
    "                        if len(blog_post_content) == 0:\n",
    "                            blog_post_content = get_real_blog_post_content_soup.select(\n",
    "                                'div.se-main-container')\n",
    "\n",
    "                        string = \"\"\n",
    "                        for sentence in blog_post_content[0].stripped_strings:\n",
    "                            string += \" \"+sentence.replace('\\xa0', \" \")\n",
    "                        # 비언어 텍스트제거\n",
    "                        string = del_outword(string)\n",
    "                        strings.append([string])\n",
    "                        count += 1\n",
    "                    except:\n",
    "                        strings.append(['X'])\n",
    "                        count += 1\n",
    "\n",
    "                except Exception as ex:\n",
    "                    #print('가져오기에러 {num}번째'.format(num = count),ex)\n",
    "                    output_error.append(count)\n",
    "                    count += 1\n",
    "                    pass\n",
    "        else:\n",
    "            #print('연결오류 {num}번째'.format(num = count),response.status_code)\n",
    "            connection_error.append(count)\n",
    "            count += 1\n",
    "\n",
    "    out_length = len(output_error+connection_error)\n",
    "    print(\"검색한 길이:\", end_num-start_num+10)\n",
    "    print(\"제외된 길이:\", out_length)\n",
    "    print(\"검색된 길이:\", end_num-start_num+10-out_length)\n",
    "    crawling_df = pd.DataFrame(\n",
    "        {\"post_dates\": postdates, \"title\": titles, \"full_text\": strings, \"url\": urls})\n",
    "#     crawling_df['post_dates'] = crawling_df['post_dates'].apply(lambda x : time_change(x))\n",
    "    return crawling_df, output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:20:05.500034Z",
     "start_time": "2020-04-01T15:20:05.498034Z"
    }
   },
   "outputs": [],
   "source": [
    "keyword = \"양양\"\n",
    "start_num = 1\n",
    "end_num = 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:23:00.616898Z",
     "start_time": "2020-04-01T15:20:05.968861Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\try00\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5ab4564a564be4aaecc2fbbb184a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='page work', max=101.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "검색한 길이: 1010\n",
      "제외된 길이: 184\n",
      "검색된 길이: 826\n"
     ]
    }
   ],
   "source": [
    "crawling_df, output_error = naver_blog_crawling(keyword = keyword, start_num = start_num,end_num = end_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:42:51.454496Z",
     "start_time": "2020-04-01T15:42:51.439492Z"
    }
   },
   "outputs": [],
   "source": [
    "crawling_df['post_dates'] =crawling_df['post_dates'].apply(lambda x : time_change(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df dt속성들\n",
    "https://m.blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221603462366&proxyReferer=https%3A%2F%2Fwww.google.com%2F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:46:47.440827Z",
     "start_time": "2020-04-01T15:46:47.436826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020    461\n",
       "2019    361\n",
       "2018      4\n",
       "Name: post_dates, dtype: int64"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawling_df['post_dates'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:47:02.490875Z",
     "start_time": "2020-04-01T15:47:02.485875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     180\n",
       "1     155\n",
       "2     119\n",
       "12    106\n",
       "11     88\n",
       "10     58\n",
       "8      24\n",
       "7      24\n",
       "9      23\n",
       "4      20\n",
       "6      19\n",
       "5      10\n",
       "Name: post_dates, dtype: int64"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawling_df['post_dates'].dt.month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:09:59.910340Z",
     "start_time": "2020-04-01T14:09:59.908340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(823, 4)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawling_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:38:19.557437Z",
     "start_time": "2020-04-01T14:38:19.510425Z"
    }
   },
   "outputs": [],
   "source": [
    "crawling_df.to_csv(keyword+\"_\"+str(end_num)+'.csv',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검수용코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:18:17.224714Z",
     "start_time": "2020-04-01T13:18:16.859239Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#1개코드 \n",
    "#저장위치\n",
    "postdates = []\n",
    "strings = []\n",
    "urls = []\n",
    "titles = []\n",
    "output_error = []\n",
    "connection_error = []\n",
    "count = 1\n",
    "\n",
    "pattern =re.compile('href=\"'+'[A-z0-9\\:\\/\\&\\;\\.\\?\\=]+')\n",
    "#keyword와 시작넘버만 바꾸면서 진행하게끔\n",
    "base_url = 'https://search.naver.com/search.naver?date_from=&date_option=0&date_to=&dup_remove=1&nso=&post_blogurl=&post_blogurl_without=&query={keyword}&sm=tab_pge&srchby=all&st=sim&where=post&start={start_num}'\n",
    "\n",
    "#for문 돌려야하는 부분(각 검색결과의 시작은 1이고, 10개씩 보여짐,)\n",
    "search_list = base_url.format(keyword = \"양양\",start_num = 11)\n",
    "response = requests.get(search_list)\n",
    "\n",
    "if response.status_code is 200:\n",
    "    #1번 검색시 10개의 결과가 출력 따라서 section은 총 10개\n",
    "    soup = BeautifulSoup(response.content,'lxml')\n",
    "    sections = soup.findAll('li', attrs={'class':'sh_blog_top'})\n",
    "    section = sections[0]\n",
    "    \n",
    "    #href부분만 가져오기 어려워서 정규표현식으로 검색\n",
    "    url = re.findall(pattern, str(section))[0].replace('?Redirect=Log&amp;logNo=','/').replace('href=\"','')\n",
    "    title = section.select_one('a.txt84').text\n",
    "    date = section.select_one('dd.txt_inline').text.strip()\n",
    "\n",
    "    #블로그 url안에 들어가기(아직 크롤링불가)\n",
    "    get_blog_post_content_code = requests.get(url)\n",
    "    get_blog_post_content_text = get_blog_post_content_code.text\n",
    "    get_blog_post_content_soup = BeautifulSoup(get_blog_post_content_text, 'lxml')\n",
    "\n",
    "    #크롤링가능한 url에 접속\n",
    "    real_blog_post_url = \"http://blog.naver.com\" + get_blog_post_content_soup.select('#mainFrame')[0].get('src')\n",
    "    get_real_blog_post_content_code = requests.get(real_blog_post_url)\n",
    "    get_real_blog_post_content_text = get_real_blog_post_content_code.text\n",
    "    get_real_blog_post_content_soup = BeautifulSoup(get_real_blog_post_content_text, 'lxml')\n",
    "\n",
    "    # url (에러나면 위에서부터 에러남)\n",
    "    urls.append(real_blog_post_url)\n",
    "    #블로그명\n",
    "    titles.append(title)\n",
    "    #날짜\n",
    "    postdates.append(date)\n",
    "\n",
    "    #본문  (postviewarea 패턴과 se-main-container 2가지 유형이 있어 분리함) \n",
    "    try:\n",
    "        blog_post_content = get_real_blog_post_content_soup.select('div#postViewArea')\n",
    "        if len(blog_post_content) == 0:\n",
    "            blog_post_content = get_real_blog_post_content_soup.select('div.se-main-container')\n",
    "\n",
    "        string = \"\"\n",
    "        for sentence in blog_post_content[0].stripped_strings:\n",
    "            string += \" \"+sentence.replace('\\xa0',\" \")\n",
    "            \n",
    "        strings.append([string])\n",
    "        count += 1\n",
    "    except:\n",
    "        strings.append([np.NaN])\n",
    "        count += 1\n",
    "\n",
    "\n",
    "# crawling_df = pd.DataFrame({\"post_dates\":postdates, \"title\":titles, \"full_text\":strings, \"url\":urls})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:23:26.391493Z",
     "start_time": "2020-04-01T13:23:26.388492Z"
    }
   },
   "outputs": [],
   "source": [
    "keyword = \"양양\"\n",
    "start_num = 1\n",
    "end_num = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:25:35.800842Z",
     "start_time": "2020-04-01T13:25:35.786839Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\try00\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7038d182bf454825841552ddd92dabac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='page work', max=2.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(start_num,end_num+1,10),desc = \"page work\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:27:26.919276Z",
     "start_time": "2020-04-01T13:27:26.917276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://search.naver.com/search.naver?date_from=&date_option=&date_to=&dup_remove=1&nso=&post_blogurl=&post_blogurl_without=&query=양양&sm=tab_pge&srchby=all&st=sim&where=post&start=11'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수화이전 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:34:01.439266Z",
     "start_time": "2020-04-01T13:34:01.432266Z"
    }
   },
   "outputs": [],
   "source": [
    "searched_urls = []\n",
    "for section in sections:\n",
    "    url = re.findall(pattern, str(section))[0].replace('?Redirect=Log&amp;logNo=','/').replace('href=\"','')\n",
    "    searched_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:45:05.896388Z",
     "start_time": "2020-04-01T13:45:02.901133Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\try00\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27face79632474ba74118c812af05ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='page work', max=2.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가져오기에러 1번째 list index out of range\n",
      "가져오기에러 2번째 list index out of range\n",
      "가져오기에러 5번째 list index out of range\n",
      "가져오기에러 7번째 list index out of range\n",
      "가져오기에러 8번째 list index out of range\n",
      "가져오기에러 13번째 list index out of range\n",
      "가져오기에러 18번째 list index out of range\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#url 찾는 패턴\n",
    "pattern =re.compile('href=\"'+'[A-z0-9\\:\\/\\&\\;\\.\\?\\=]+')\n",
    "\n",
    "#저장위치\n",
    "postdates = []\n",
    "strings = []\n",
    "urls = []\n",
    "titles = []\n",
    "output_error = []\n",
    "connection_error = []\n",
    "count = 1\n",
    "\n",
    "#keyword와 시작넘버만 바꾸면서 진행하게끔\n",
    "base_url = 'https://search.naver.com/search.naver?date_from=&date_option=&date_to=&dup_remove=1&nso=&post_blogurl=&post_blogurl_without=&query={keyword}&sm=tab_pge&srchby=all&st=sim&where=post&start={start}'\n",
    "\n",
    "#for문 돌려야하는 부분(각 검색결과의 시작은 1이고, 10개씩 보여짐,)\n",
    "for i in tqdm_notebook(range(start_num,end_num+1,10),desc = \"page work\"):\n",
    "    search_list = base_url.format(keyword = keyword,start = i)\n",
    "    response = requests.get(search_list)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        #1번 검색시 10개의 결과가 출력 따라서 section은 총 10개\n",
    "        soup = BeautifulSoup(response.content,'lxml')\n",
    "        sections = soup.findAll('li', attrs={'class':'sh_blog_top'})\n",
    "\n",
    "        for section in sections:\n",
    "            try:\n",
    "                #href부분만 가져오기 어려워서 정규표현식으로 검색\n",
    "                url = re.findall(pattern, str(section))[0].replace('?Redirect=Log&amp;logNo=','/').replace('href=\"','')\n",
    "                title = section.select_one('a.txt84').text\n",
    "                date = section.select_one('dd.txt_inline').text.strip()\n",
    "\n",
    "                #블로그 url안에 들어가기(아직 크롤링불가)\n",
    "                get_blog_post_content_code = requests.get(url)\n",
    "                get_blog_post_content_text = get_blog_post_content_code.text\n",
    "                get_blog_post_content_soup = BeautifulSoup(get_blog_post_content_text, 'lxml')\n",
    "\n",
    "                #크롤링가능한 url에 접속\n",
    "                real_blog_post_url = \"http://blog.naver.com\" + get_blog_post_content_soup.select('#mainFrame')[0].get('src')\n",
    "                get_real_blog_post_content_code = requests.get(real_blog_post_url)\n",
    "                get_real_blog_post_content_text = get_real_blog_post_content_code.text\n",
    "                get_real_blog_post_content_soup = BeautifulSoup(get_real_blog_post_content_text, 'lxml')\n",
    "\n",
    "                # url (에러나면 위에서부터 에러남)\n",
    "                urls.append(real_blog_post_url)\n",
    "                #블로그명\n",
    "                titles.append(title)\n",
    "                #날짜\n",
    "                postdates.append(date)\n",
    "\n",
    "                #본문  (postviewarea 패턴과 se-main-container 2가지 유형이 있어 분리함) \n",
    "                try:\n",
    "                    blog_post_content = get_real_blog_post_content_soup.select('div#postViewArea')\n",
    "                    if len(blog_post_content) == 0:\n",
    "                        blog_post_content = get_real_blog_post_content_soup.select('div.se-main-container')\n",
    "\n",
    "                    string = \"\"\n",
    "                    for sentence in blog_post_content[0].stripped_strings:\n",
    "                        string += \" \"+sentence.replace('\\xa0',\" \")\n",
    "                    #비언어 텍스트제거 \n",
    "                    string = del_outword(string)\n",
    "                    strings.append([string]) \n",
    "                    count += 1\n",
    "                except:\n",
    "                    strings.append([np.NaN])\n",
    "                    count += 1\n",
    "                    pass\n",
    "\n",
    "            except Exception as ex: \n",
    "                print('가져오기에러 {num}번째'.format(num = count),ex)\n",
    "                output_error.append(count)\n",
    "                count += 1\n",
    "                pass\n",
    "    else:\n",
    "        print('연결오류 {num}번째'.format(num = count),response.status_code)\n",
    "        connection_error.append(count)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2020-04-01 17:04:35 길이가 다른것에 해결책\n",
    "* 에러나면 NAN추가해버리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T01:28:42.176578Z",
     "start_time": "2020-04-01T01:28:42.167585Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#이전코드\n",
    "def get_blog_post(keyword, display_count, page_count, sort_type,save = True):\n",
    "    '''\n",
    "    keyword : 검색하고 싶은 키워드\n",
    "    display_count : 한 페이지당 표출할 개수 min = 10, max = 100\n",
    "    page_count : 총 진행할 페이지의 수 min = 1, max = 1000\n",
    "    sort_type : 정렬옵션 \"sim\" (유사도순), \"date\" (날짜순)\n",
    "    \n",
    "    display_count는 표시되는 개수에 불과하며 page_count가 1개씩 넘어갈때마다 나머지 diplay_count가 중복됨\n",
    "    ''' \n",
    "    #날짜와 본문내용 저장\n",
    "    postdates = []\n",
    "    strings = []\n",
    "    urls = []\n",
    "    titles = []\n",
    "    blogger_names = []\n",
    "    responses = []\n",
    "\n",
    "    encode_keyword = urllib.parse.quote(keyword)\n",
    "    # get_blog_search_result_pagination_count로 처리할 수있는 페이지수를 1부터 페이지수까지 까지 각각 하나씩 긁어옴 \n",
    "    for i in tqdm_notebook(range(1, page_count + 1),desc = \"page work\"):\n",
    "        url = \"https://openapi.naver.com/v1/search/blog?query=\" + encode_keyword + \"&display=\" + str(\n",
    "            display_count) + \"&start=\" + str(i) + \"&sort=\" + sort_type\n",
    "\n",
    "        request = urllib.request.Request(url)\n",
    "\n",
    "        request.add_header(\"X-Naver-Client-Id\", naver_client_id)\n",
    "        request.add_header(\"X-Naver-Client-Secret\", naver_client_secret)\n",
    "\n",
    "        #접속오류났을때 종료후 저장\n",
    "        try:\n",
    "            response = urllib.request.urlopen(request)\n",
    "            response_code = response.getcode()\n",
    "\n",
    "            # respon결과가 200일때 결과읽어옴 \n",
    "            if response_code is 200:\n",
    "                response_body = response.read()\n",
    "                response_body_dict = json.loads(response_body.decode('utf-8'))\n",
    "                responses.append(response_body_dict)\n",
    "                try:\n",
    "                #items의 개수만큼씩 진행\n",
    "                    for j in range(0, len(response_body_dict['items'])):\n",
    "                        blog_post_url = response_body_dict['items'][j]['link'].replace(\"amp;\", \"\")\n",
    "\n",
    "                        #블로그 url안에 들어가기(아직 크롤링불가)\n",
    "                        get_blog_post_content_code = requests.get(blog_post_url)\n",
    "                        get_blog_post_content_text = get_blog_post_content_code.text\n",
    "                        get_blog_post_content_soup = BeautifulSoup(get_blog_post_content_text, 'lxml')\n",
    "                        #크롤링가능한 url에 접속\n",
    "                        real_blog_post_url = \"http://blog.naver.com\" + get_blog_post_content_soup.select('#mainFrame')[0].get('src')\n",
    "                        get_real_blog_post_content_code = requests.get(real_blog_post_url)\n",
    "                        get_real_blog_post_content_text = get_real_blog_post_content_code.text\n",
    "                        get_real_blog_post_content_soup = BeautifulSoup(get_real_blog_post_content_text, 'lxml')\n",
    "                        #본문부분 추출 \n",
    "                        blog_post_content = get_real_blog_post_content_soup.select('div#postViewArea')\n",
    "                        if len(blog_post_content) == 0:\n",
    "                            blog_post_content = get_real_blog_post_content_soup.select('div.se-main-container')\n",
    "\n",
    "                        #포스트날짜\n",
    "                        postdate = datetime.datetime.strptime(response_body_dict['items'][j]['postdate'],\"%Y%m%d\").strftime(\"%y.%m.%d\")\n",
    "                        postdates.append(postdate)\n",
    "\n",
    "                        #제목\n",
    "                        remove_html_tag = re.compile('<.*?>')\n",
    "                        title = re.sub(remove_html_tag, '', response_body_dict['items'][j]['title'])\n",
    "                        titles.append(title)\n",
    "\n",
    "                        #블로거이름\n",
    "                        blogger_name = response_body_dict['items'][j]['bloggername']\n",
    "                        blogger_names.append(blogger_name)\n",
    "\n",
    "                        #전체 텍스트 \n",
    "                        string = \"\"\n",
    "                        for sentence in blog_post_content[0].stripped_strings:\n",
    "                            string += \" \"+sentence.replace('\\xa0',\" \")\n",
    "                        #비언어 텍스트제거 \n",
    "                        string = del_outword(string)\n",
    "                        strings.append([string]) \n",
    "\n",
    "                        #url\n",
    "                        urls.append(real_blog_post_url)\n",
    "\n",
    "                except Exception as ex:\n",
    "                    print('추출에러 {i}번째'.format(i),ex)\n",
    "                    pass\n",
    "            else :\n",
    "                pass\n",
    "\n",
    "        except Exception as ex:\n",
    "            print('open에러 {i}번째'.format(i),ex)\n",
    "            pass\n",
    "                \n",
    "    # utf-8형식으로 저장하면 엑셀에서 열때 에러나지만, load는 가능\n",
    "    if save == True:\n",
    "        crawling_df = pd.DataFrame({\"post_dates\":postdates, \"title\":titles, \"blogger_name\":blogger_names, \"full_text\":strings, \"url\":urls})\n",
    "        crawling_df.to_csv(keyword+\"_\"+str(display_count* page_count)+\".csv\",encoding='utf-8',index= False)\n",
    "    return crawling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T01:36:33.222309Z",
     "start_time": "2020-04-01T01:28:42.853827Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e21cc6074f450dbd0aff67c17f41bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='page work', max=1000.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-9f53351215b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcrawling_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_blog_post\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-7f3a76e12a66>\u001b[0m in \u001b[0;36mget_blog_post\u001b[1;34m(keyword, display_count, page_count, sort_type, save)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;31m# utf-8형식으로 저장하면 엑셀에서 열때 에러나지만, load는 가능\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mcrawling_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"post_dates\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpostdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"title\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"blogger_name\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mblogger_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"full_text\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"url\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mcrawling_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisplay_count\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mpage_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcrawling_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    409\u001b[0m             )\n\u001b[0;32m    410\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         ]\n\u001b[1;32m--> 257\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "crawling_df = get_blog_post(keyword, display_count, page_count, sort_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연관분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:11:25.963192Z",
     "start_time": "2020-04-01T14:11:25.956192Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crawling_df_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-b096d16bff44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# crawling_df_s = pd.read_csv('양양_1000.csv',encoding='utf-8')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcrawling_df_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'crawling_df_s' is not defined"
     ]
    }
   ],
   "source": [
    "# crawling_df_s = pd.read_csv('양양_1000.csv',encoding='utf-8')\n",
    "crawling_df_s['title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:11:32.908059Z",
     "start_time": "2020-04-01T14:11:31.899833Z"
    }
   },
   "outputs": [],
   "source": [
    "komoran = Komoran(userdic='user_dictionary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#이모티콘 제거 [출처](https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:11:34.178347Z",
     "start_time": "2020-04-01T14:11:34.172346Z"
    }
   },
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "#분석에 어긋나는 불용어구 제외 (특수문자, 의성어)\n",
    "han = re.compile(r'[ㄱ-ㅎㅏ-ㅣ!?~,\".\\n\\r#\\ufeff\\u200d]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:36:23.316238Z",
     "start_time": "2020-04-01T14:36:05.691428Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\try00\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88accb396ce9418986a34a9eaf035492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=824.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sw = ['오늘','시간','최근','지난','관련','이번','이날','현재','10','올해','지난해','있다','이상','기준','때문',\n",
    "     \"메뉴\",\"주문\",\"강원도\",'정도','도착','생각','양양군','사진','위치']\n",
    "all_nouns =[]\n",
    "\n",
    "for i in tqdm_notebook(range(0,len(crawling_df))):\n",
    "    try:\n",
    "        nouns = komoran.nouns(crawling_df['full_text'][i][0])\n",
    "    except :\n",
    "        nouns = crawling_df['full_text'][i][0].replace('\\u200b',\" \")\n",
    "        nouns = emoji_pattern.sub(r'',nouns)\n",
    "        nouns = han.sub(r'',TEXT_refine)\n",
    "    clean_nouns = list(filter(lambda word : (len(word)>1)&(word not in sw),nouns))\n",
    "    all_nouns.append(clean_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:51:18.864395Z",
     "start_time": "2020-04-01T14:51:18.861394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 동절기 숙소 당첨으로 다녀온 양양 주말엔 예약하기 쉽지 않던 양양솔비치 버스타고 혼자 잘 다녀왔다 처음으로 국내 장거리 뚜벅이여행 을 했는데 할만하다 + 세상못할짓이다 모두를 경험했다 어쨌거나 숙소 후기부터 시작하자 이게 호텔이고 주변으로 빌라형이 있었음 양양솔비치는 예전에 있던 숙소를 개조()한 거라 낡은 느낌이 많다고 하는 후기가 많았는데 뭐 그냥 적당했다 막 낡아서 서운할정도는 아니다 이런얘기를 못듣고 갔으면 눈치챌 여력도 없었을듯  원래 온돌룸으로 예약했는데(혼자갈 생각아니었음) 혼자간김에 침대룸으로 바꾸었다 다행히 가능 리셉션에 있던 직원들은 친절했다 여기는 예약을 해두면 오는 순서대로 같은 레벨 중 좋은()방을 배정해주는 듯 했다 그러니 체크인 시간 맞춰서 빨리 후닥 가십시다 더블싱글 침대 두개 혼자쓰기 산전망이라서 베란다 나가면 이런뷰 보입니다 방은 작다 암막커튼을 걷지 않으면 갑갑하다 출장지에서 슈페리어 룸으로 많이들 가는데 비교해보면 역시 작다 비교하기 나름이라지만 양양이 싱가폴이나 런던은 아니잖;; 온돌방이 방사이즈가 같다는데 정돈 안된 갑갑함을 경험할뻔 했다 바꾸길 잘했다 호텔이라서 국내에서 자주가던 콘도와 다르다 클린형이라더니 취사도구가 전혀없다 그냥 해외 호텔 생각하면 된다 전기포트는 있다 각종 잔도 다 있고 아주 비싼 미니바는 전세계 공통 이 정보를 미리 알았기에 미리 시내에서 내가 먹을 맥주는 사가지고 들어갔다 내부 매점도 당연히 비쌈 각종 술잔 구비 작은 미니바용 냉장고 있고요 미니바 가격 캔콜라 하나 3천원은너무하잖니 화장대 하나 있고 드라이기가 있긴한데 머리 말리다 인내심 테스트 당하는줄 알고 두리번 이방만 고장났었던진 모르겠으나 찬바람 약한뜨거운바람까지 나오고 강한 뜨거운바람 불가한 드라이기망가진걸까대체 어떻게 화장실도 깔끔하다 어메니티는 Aveda 민트샴푸가 시원했다 수압이 좀 약해서 별로였는데 욕실 구배가 잘못된듯 샤워하고 나왔더니 욕실밖으로 물이 많이 넘쳤()는데 하마터면 방쪽 카펫으로 물 넘칠뻔 했다 샴푸든 수건이든 부족한건 후다닥 가져다 준다 직원들은 진짜 친절하더라 2005년 산불의 때문인지 양양 곳곳엔 소방설비가 잘 되어 있느거 같았는데 호텔안에도 큰 소화기  입실할때 뭔 쿠폰을 잔뜩 많이 주지만 하나도 안씀 나만 안쓴건지 보통 안쓴건지 모르겠지만 이걸 다 쓰고 나가는 손님은 단언컨대 없을듯 차라리 그냥 시스템에 룸 넘버 입력하면 조회되는 그런 친환경 시스템은 어떨지 참고로 사우나는 갈까 했더니 정가는 13천원 10%할인해도 11700원 위메프에서 더 싸게 쿠폰을 팔더라 6천원  숙박 자체에 불만이 있었던건 전혀 아니다 침대 푹신했고 직원 친절했고 바다 예뻤고  하지만 해외 호텔 생활을 너무 했나 호텔 가성비를 아무리 생각해 봐도 비싸다 주말요금이 뭐 일박에 30만원도 하는것 같던데 싸게 잡아도 20만원은 하는거 같은데 과연 그값을 하고있는가 잠시 생각했다 굳이대체왜 게다가 주중과 주말의 가격 격차가 너무 심한듯  게다가 위치가 여긴 차 없으면 오면 고생문 호텔안에서만 뒹굴생각이 아니면 차는 필수다 양양터미널에서 택시로 들어와도 9천원 정도고 주변 어디 가까운 수산항에 밥이라도 먹으러 나가려면 택시는 기본이고 버스는 1시간에 1대도 제대로 없는 그런곳이니염두에 두시길 (하긴 다 차 가지고 왔긴 한거 같더라)  우리나라는 이름있는 체인들의 가격이 아무리 자본주의라지만 좀 너무 쎄다 -_-;']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawling_df.iloc[411]['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:05:03.898685Z",
     "start_time": "2020-04-01T15:05:03.896685Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:05:11.715448Z",
     "start_time": "2020-04-01T15:05:11.713448Z"
    }
   },
   "outputs": [],
   "source": [
    "today = datetime.datetime.today().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:05:34.941859Z",
     "start_time": "2020-04-01T15:05:34.939859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2020, 4, 1)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today - timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:15:54.532567Z",
     "start_time": "2020-04-01T15:15:54.515565Z"
    }
   },
   "outputs": [],
   "source": [
    "crawling_df['post_dates'] = crawling_df['post_dates'].apply(lambda x : time_change(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:18:52.946795Z",
     "start_time": "2020-04-01T15:18:52.940793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020-03-07    765\n",
       "2020-03-31     19\n",
       "2020-04-01     10\n",
       "2020-03-26      8\n",
       "2020-03-29      7\n",
       "2020-03-27      6\n",
       "2020-03-30      5\n",
       "2020-03-28      4\n",
       "Name: post_dates, dtype: int64"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawling_df['post_dates'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:55:23.938527Z",
     "start_time": "2020-04-01T14:55:23.932535Z"
    }
   },
   "outputs": [],
   "source": [
    "#all_noun unlist\n",
    "aa = [y for x in all_nouns for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:00:34.733010Z",
     "start_time": "2020-04-01T15:00:34.704002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "양양     5547\n",
       "바다     1496\n",
       "여행     1310\n",
       "서핑     1005\n",
       "해변      736\n",
       "맛집      725\n",
       "낙산사     723\n",
       "카페      722\n",
       "속초      713\n",
       "사람      695\n",
       "비치      573\n",
       "쏠비치     566\n",
       "호텔      521\n",
       "느낌      515\n",
       "겨울      492\n",
       "송이      467\n",
       "조대      451\n",
       "모습      430\n",
       "연어      426\n",
       "마음      424\n",
       "dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(aa).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:36:23.633309Z",
     "start_time": "2020-04-01T14:36:23.627308Z"
    }
   },
   "outputs": [],
   "source": [
    "noun_df = pd.DataFrame({'nouns':all_nouns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:36:23.792345Z",
     "start_time": "2020-04-01T14:36:23.789344Z"
    }
   },
   "outputs": [],
   "source": [
    "rules = apriori(all_nouns, min_support=0.2, min_condience =0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:36:24.053403Z",
     "start_time": "2020-04-01T14:36:23.942379Z"
    }
   },
   "outputs": [],
   "source": [
    "results  = list(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:36:24.411484Z",
     "start_time": "2020-04-01T14:36:24.204438Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns =[\"lhs\",'rhs','support','confidence','lift'])\n",
    "index =0\n",
    "for row in results:   \n",
    "    support = row[1]\n",
    "    ordered_stat = row[2]\n",
    "    for ordered_item in ordered_stat:\n",
    "        lhs = \" \".join(x for x in ordered_item[0])\n",
    "        rhs = \" \".join(x for x in ordered_item[1])\n",
    "        confidence = ordered_item[2]\n",
    "        lift = ordered_item[3]\n",
    "        results_df.loc[index] = [lhs,rhs,support,confidence,lift]\n",
    "        index = index +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:36:24.566520Z",
     "start_time": "2020-04-01T14:36:24.564519Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df = results_df.sort_values(by=\"confidence\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:36:24.727556Z",
     "start_time": "2020-04-01T14:36:24.719555Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lhs</th>\n",
       "      <th>rhs</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>모습</td>\n",
       "      <td>양양</td>\n",
       "      <td>0.241505</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.011043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>날씨</td>\n",
       "      <td>양양</td>\n",
       "      <td>0.228155</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.011043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>커피</td>\n",
       "      <td>양양</td>\n",
       "      <td>0.201456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.011043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>맛집</td>\n",
       "      <td>양양</td>\n",
       "      <td>0.259709</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.011043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>여행 느낌</td>\n",
       "      <td>양양</td>\n",
       "      <td>0.201456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.011043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>거리</td>\n",
       "      <td>0.202670</td>\n",
       "      <td>0.202670</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td></td>\n",
       "      <td>여행 느낌</td>\n",
       "      <td>0.201456</td>\n",
       "      <td>0.201456</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td></td>\n",
       "      <td>양양 여행 느낌</td>\n",
       "      <td>0.201456</td>\n",
       "      <td>0.201456</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td></td>\n",
       "      <td>커피</td>\n",
       "      <td>0.201456</td>\n",
       "      <td>0.201456</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td></td>\n",
       "      <td>커피 양양</td>\n",
       "      <td>0.201456</td>\n",
       "      <td>0.201456</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lhs       rhs   support  confidence      lift\n",
       "64      모습        양양  0.241505    1.000000  1.011043\n",
       "43      날씨        양양  0.228155    1.000000  1.011043\n",
       "116     커피        양양  0.201456    1.000000  1.011043\n",
       "61      맛집        양양  0.259709    1.000000  1.011043\n",
       "125  여행 느낌        양양  0.201456    1.000000  1.011043\n",
       "..     ...       ...       ...         ...       ...\n",
       "2                 거리  0.202670    0.202670  1.000000\n",
       "48             여행 느낌  0.201456    0.201456  1.000000\n",
       "120         양양 여행 느낌  0.201456    0.201456  1.000000\n",
       "25                커피  0.201456    0.201456  1.000000\n",
       "114            커피 양양  0.201456    0.201456  1.000000\n",
       "\n",
       "[155 rows x 5 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
