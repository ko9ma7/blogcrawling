{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1. 라이브러리 설치 및 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "카카오 \n",
    "\n",
    "네이티브 앱 키\n",
    "\n",
    "REST API 키\n",
    "\n",
    "JavaScript 키\n",
    "\n",
    "Admin 키\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4==4.7.1 in /Users/spark/miniconda3/envs/study37/lib/python3.6/site-packages (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /Users/spark/miniconda3/envs/study37/lib/python3.6/site-packages (from beautifulsoup4==4.7.1) (1.9.2)\n",
      "Requirement already satisfied: selenium==3.11.0 in /Users/spark/miniconda3/envs/study37/lib/python3.6/site-packages (3.11.0)\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리의 버전을 맞춰주셔야 합니다. (특히 selenium은 꼭 버전을 맞춰 설치해주세요!)\n",
    "\n",
    "!pip install beautifulsoup4==4.7.1\n",
    "!pip install selenium==3.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2. 카카오 API 를 통한 요청 및 응답 엑셀파일 저장 (본문 중 일부)\n",
    "> 아래의 2-2 및 2-3 에서 확인하실 수 있듯 **main_func 함수**를 호출하여 원하는 정보만 입력하면 저장된 엑셀 파일을 확인하실 수 있습니다.<br><br>\n",
    "**한번에 너무 많은 페이지를 요청하고 저장하게되면 아래 3번 항목에서 전체 본문을 크롤링하고 저장할 때 무리가 있습니다.**<br>\n",
    "적당하게 10 페이지 이내로 저장하고 저장된 엑셀 파일을 바탕으로 3번 항목에서 전체 본문을 크롤링해서 저장해보시며 페이지 수를 늘려보세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2-1. 카카오 API 활용을 위한 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MYAPP_KEY = 'd9bREST KEY를입력해주세요5f6d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 검색 대상(web/cafe/blog/tip/book) 및 키워드 등의 정보를 바탕으로 API 요청을 위한 URL을 구성합니다.\n",
    "def gen_search_url(api_node, search_text, start_num, disp_num):\n",
    "    \n",
    "    if api_node == 'book':\n",
    "        base = \"https://dapi.kakao.com/v3/search\"\n",
    "    else:\n",
    "        base = \"https://dapi.kakao.com/v2/search\"\n",
    "    \n",
    "    node = \"/\" + api_node + \".json\"\n",
    "    param_query = \"?query=\" + urllib.parse.quote(search_text)\n",
    "    param_start = \"&page=\" + str(start_num)\n",
    "    param_disp = \"&size=\" + str(disp_num)\n",
    "    \n",
    "    return base + node + param_query + param_start + param_disp\n",
    "\n",
    "\n",
    "# 구성이 완료된 URL을 바탕으로 API 요청을 진행하고 json type(python dict)의 응답을 받아옵니다.\n",
    "def get_result_onpage(url):\n",
    "    \n",
    "    request = urllib.request.Request(url)\n",
    "    header = {'Authorization': 'KakaoAK {}'.format(MYAPP_KEY)}\n",
    "    response = requests.post(url, headers=header)\n",
    "    print(\"[%s] url Request Success\" % datetime.datetime.now())\n",
    "    \n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "# 주어진 str에서 HTML 태그들을 모두 삭제하고, 그 외 불필요한 기호를 삭제합니다.\n",
    "def delete_tag(input_str):\n",
    "    pattern = re.compile(r'<.*?>') # HTML 태그들을 모두 삭제하기 위한 정규표현식 패턴\n",
    "    input_str = pattern.sub('', input_str)\n",
    "    input_str = input_str.replace(\"&lt;\",\"\")\n",
    "    input_str = input_str.replace(\"&gt;\",\"\")\n",
    "    input_str = input_str.replace(\"&amp;\",\"\")\n",
    "    \n",
    "    return input_str\n",
    "\n",
    "\n",
    "# 주어진 json type의 API 응답 데이터에서 웹페이지별 전체 데이터를 꺼내어 DataFrame에 담는 함수입니다.\n",
    "def doc_to_df(search_target, json_data):\n",
    "    \n",
    "    # 'tip' 의 경우는 응답으로 주어지는 URL이 질문과 답변에 대한 2가지 URL입니다.\n",
    "    if search_target == 'tip':\n",
    "        titles = [delete_tag(each['title']) for each in json_data['documents']]\n",
    "        contents = [delete_tag(each['contents']) for each in json_data['documents']]    \n",
    "        q_urls = [each['q_url'] for each in json_data['documents']]\n",
    "        a_urls = [each['a_url'] for each in json_data['documents']]        \n",
    "        datetimes = [each['datetime'][:10] + ', ' + each['datetime'][11:19] for each in json_data['documents']]\n",
    "        \n",
    "        result_pd = pd.DataFrame({'titles':titles,\n",
    "                                  'contents':contents, \n",
    "                                  'urls_question':q_urls, \n",
    "                                  'urls_answer':a_urls, \n",
    "                                  'datetimes':datetimes}, \n",
    "                                 columns=['titles', 'contents', 'urls_question', 'urls_answer', 'datetimes'])\n",
    "    \n",
    "    else:\n",
    "        titles = [delete_tag(each['title']) for each in json_data['documents']]\n",
    "        contents = [delete_tag(each['contents']) for each in json_data['documents']]    \n",
    "        urls = [each['url'] for each in json_data['documents']]\n",
    "        datetimes = [each['datetime'][:10] + ', ' + each['datetime'][11:19] for each in json_data['documents']]\n",
    "        \n",
    "        result_pd = pd.DataFrame({'titles':titles,\n",
    "                                  'contents':contents, \n",
    "                                  'urls':urls, \n",
    "                                  'datetimes':datetimes}, \n",
    "                                 columns=['titles', 'contents', 'urls', 'datetimes'])\n",
    "    return result_pd\n",
    "\n",
    "\n",
    "# 주어진 검색 대상(web/cafe/blog/tip/book) 및 키워드 등의 정보를 바탕으로 크롤링을 진행하는 메인 함수입니다.\n",
    "def crawl_all(search_target, keyword, max_page, doc_per_page):\n",
    "    \n",
    "    # 첫번째 페이지에 대한 결과 데이터로 base dataframe을 생성합니다.\n",
    "    url = gen_search_url(search_target, keyword, 1, doc_per_page) \n",
    "    one_result = get_result_onpage(url)\n",
    "    main_df = doc_to_df(search_target, one_result)\n",
    "    \n",
    "    # 두번째 페이지부터 크롤링 후 DataFrame merge 작업을 진행하며, 마지막 페이지까지만 크롤링 후 함수를 종료합니다.\n",
    "    for page in range(2, max_page+1):\n",
    "        url = gen_search_url(search_target, keyword, page, doc_per_page)\n",
    "        print('Getting data from : {}'.format(url))\n",
    "        one_result = get_result_onpage(url)\n",
    "        \n",
    "        if one_result['meta']['is_end'] == True:\n",
    "            print('\\nURL Response의 (meta)is_end 값이 True입니다. 마지막 페이지인 현재 페이지까지만 수합합니다.')\n",
    "            temp_df = doc_to_df(search_target, one_result)\n",
    "            main_df = pd.concat([main_df, temp_df], axis=0)\n",
    "            \n",
    "            return main_df\n",
    "    \n",
    "        else:\n",
    "            temp_df = doc_to_df(search_target, one_result)\n",
    "            main_df = pd.concat([main_df, temp_df], axis=0)\n",
    "\n",
    "        # 단 시간 내의 빈번한 요청으로 인해 API 요청이 막히는 것을 방지하기 위한 sleep입니다.\n",
    "        time.sleep(5)\n",
    "    \n",
    "    return main_df\n",
    "    \n",
    "    \n",
    "# 사용자로부터 크롤링을 위한 정보를 받아들이고 메인 크롤링 함수를 실행한 후 결과 DataFrame을 엑셀 파일로 저장합니다.\n",
    "# 엑셀 파일의 형식은 'result_크롤링분야_연월일_시분.xlsx' 입니다.\n",
    "# 엑셀 파일을 열었을 때 url 텍스트들로 인해 경고 메시지가 뜰 경우 복구 \"예\"를 클릭하시고 그대로 사용하시면 됩니다. (문제 없음)\n",
    "def main_func():\n",
    "\n",
    "    search_target = input('문서를 크롤링할 대상을 입력해주세요 (web/cafe/blog/tip/book 중 택일, 소문자) : ')\n",
    "    keyword = input('크롤링을 원하시는 키워드를 입력해주세요 : ')\n",
    "    max_page = input('크롤링을 원하시는 최대 페이지를 2~50 사이의 숫자로 입력해주세요. : ')\n",
    "    doc_per_page = input('크롤링을 원하시는 페이지 당 문서 수를 1~50 사이의 숫자로 입력해주세요. : ')\n",
    "\n",
    "    if (search_target=='') or (keyword=='') or (max_page=='') or (doc_per_page==''):\n",
    "        print('\\n입력된 값 중 제대로 입력되지 않은 값이 있습니다. 다시 실행해주세요.')\n",
    "        return False\n",
    "    \n",
    "    if search_target in ['web', 'book']:\n",
    "        print('\\n[ web ] 혹은 [ book ] 을 대상으로 크롤링 시 웹사이트 중 URL이 255자가 넘어가는 사이트는 URL이 엑셀에 저장이 되지 않습니다.')\n",
    "        print('함수 실행 후 돌려받은 DataFrame에는 그러한 웹사이트의 URL이 그대로 남아있으므로 DataFrame에서 확인하실 수 있습니다.\\n')\n",
    "              \n",
    "    max_page = int(max_page)\n",
    "    doc_per_page = int(doc_per_page)\n",
    "    \n",
    "    result_df = crawl_all(search_target, keyword, max_page, doc_per_page)\n",
    "    result_df = result_df.reset_index()\n",
    "    del result_df['index']\n",
    "    print('\\nCrawling process is finished!')\n",
    "    \n",
    "    file_name = 'result_{}_{}.xlsx'.format(search_target, datetime.datetime.now().strftime('%y%m%d_%H%M'))\n",
    "    result_df.to_excel(file_name, encoding='utf-8', index=False)\n",
    "    print('\\nCrawling result is saved at [ {} ]'.format(file_name))\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2-2. 카카오 API 활용을 통한 데이터 요청 및 엑셀 저장 예시 (cafe / 뇌과학 / 3페이지 / 페이지당 50개 문서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서를 크롤링할 대상을 입력해주세요 (web/cafe/blog/tip/book 중 택일, 소문자) : book\n",
      "크롤링을 원하시는 키워드를 입력해주세요 : 창의성\n",
      "크롤링을 원하시는 최대 페이지를 2~50 사이의 숫자로 입력해주세요. : 50\n",
      "크롤링을 원하시는 페이지 당 문서 수를 1~50 사이의 숫자로 입력해주세요. : 50\n",
      "\n",
      "[ web ] 혹은 [ book ] 을 대상으로 크롤링 시 웹사이트 중 URL이 255자가 넘어가는 사이트는 URL이 엑셀에 저장이 되지 않습니다.\n",
      "함수 실행 후 돌려받은 DataFrame에는 그러한 웹사이트의 URL이 그대로 남아있으므로 DataFrame에서 확인하실 수 있습니다.\n",
      "\n",
      "[2019-08-06 19:32:48.820631] url Request Success\n",
      "Getting data from : https://dapi.kakao.com/v3/search/book.json?query=%EC%B0%BD%EC%9D%98%EC%84%B1&page=2&size=50\n",
      "[2019-08-06 19:32:48.935861] url Request Success\n",
      "Getting data from : https://dapi.kakao.com/v3/search/book.json?query=%EC%B0%BD%EC%9D%98%EC%84%B1&page=3&size=50\n",
      "[2019-08-06 19:32:54.064013] url Request Success\n",
      "Getting data from : https://dapi.kakao.com/v3/search/book.json?query=%EC%B0%BD%EC%9D%98%EC%84%B1&page=4&size=50\n",
      "[2019-08-06 19:32:59.212581] url Request Success\n",
      "Getting data from : https://dapi.kakao.com/v3/search/book.json?query=%EC%B0%BD%EC%9D%98%EC%84%B1&page=5&size=50\n",
      "[2019-08-06 19:33:04.345048] url Request Success\n",
      "Getting data from : https://dapi.kakao.com/v3/search/book.json?query=%EC%B0%BD%EC%9D%98%EC%84%B1&page=6&size=50\n",
      "[2019-08-06 19:33:09.483358] url Request Success\n",
      "Getting data from : https://dapi.kakao.com/v3/search/book.json?query=%EC%B0%BD%EC%9D%98%EC%84%B1&page=7&size=50\n",
      "[2019-08-06 19:33:14.616992] url Request Success\n",
      "Getting data from : https://dapi.kakao.com/v3/search/book.json?query=%EC%B0%BD%EC%9D%98%EC%84%B1&page=8&size=50\n",
      "[2019-08-06 19:33:19.747638] url Request Success\n",
      "Getting data from : https://dapi.kakao.com/v3/search/book.json?query=%EC%B0%BD%EC%9D%98%EC%84%B1&page=9&size=50\n",
      "[2019-08-06 19:33:24.884718] url Request Success\n",
      "Getting data from : https://dapi.kakao.com/v3/search/book.json?query=%EC%B0%BD%EC%9D%98%EC%84%B1&page=10&size=50\n",
      "[2019-08-06 19:33:30.008839] url Request Success\n",
      "Getting data from : https://dapi.kakao.com/v3/search/book.json?query=%EC%B0%BD%EC%9D%98%EC%84%B1&page=11&size=50\n",
      "[2019-08-06 19:33:35.120478] url Request Success\n",
      "\n",
      "URL Response의 (meta)is_end 값이 True입니다. 마지막 페이지인 현재 페이지까지만 수합합니다.\n",
      "\n",
      "Crawling process is finished!\n",
      "\n",
      "Crawling result is saved at [ result_book_190806_1933.xlsx ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>contents</th>\n",
       "      <th>urls</th>\n",
       "      <th>datetimes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>인문학으로 광고하다</td>\n",
       "      <td>『인문학으로 광고하다』는 창의성에 관한 책이다. 이 책은 창의성에 대해 이야기하기 ...</td>\n",
       "      <td>https://search.daum.net/search?w=bookpage&amp;book...</td>\n",
       "      <td>2009-08-27, 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>창의성을 지휘하라</td>\n",
       "      <td>『창의성을 지휘하라』는 기업의 대표적 롤모델인 픽사와 디즈니 애니메이션의 성공신화를...</td>\n",
       "      <td>https://search.daum.net/search?w=bookpage&amp;book...</td>\n",
       "      <td>2014-09-16, 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>창의성</td>\n",
       "      <td>『창의성』은 총 10장으로 나누어 구성되어 있으며, 윤리학과 창의성의 개념을 설명하...</td>\n",
       "      <td>https://search.daum.net/search?w=bookpage&amp;book...</td>\n",
       "      <td>2012-02-29, 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>창의성</td>\n",
       "      <td></td>\n",
       "      <td>https://search.daum.net/search?w=bookpage&amp;book...</td>\n",
       "      <td>2005-03-07, 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>창의성의 즐거움(양장본 HardCover)</td>\n",
       "      <td></td>\n",
       "      <td>https://search.daum.net/search?w=bookpage&amp;book...</td>\n",
       "      <td>2003-11-28, 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    titles                                           contents  \\\n",
       "0               인문학으로 광고하다  『인문학으로 광고하다』는 창의성에 관한 책이다. 이 책은 창의성에 대해 이야기하기 ...   \n",
       "1                창의성을 지휘하라  『창의성을 지휘하라』는 기업의 대표적 롤모델인 픽사와 디즈니 애니메이션의 성공신화를...   \n",
       "2                      창의성  『창의성』은 총 10장으로 나누어 구성되어 있으며, 윤리학과 창의성의 개념을 설명하...   \n",
       "3                      창의성                                                      \n",
       "4  창의성의 즐거움(양장본 HardCover)                                                      \n",
       "\n",
       "                                                urls             datetimes  \n",
       "0  https://search.daum.net/search?w=bookpage&book...  2009-08-27, 00:00:00  \n",
       "1  https://search.daum.net/search?w=bookpage&book...  2014-09-16, 00:00:00  \n",
       "2  https://search.daum.net/search?w=bookpage&book...  2012-02-29, 00:00:00  \n",
       "3  https://search.daum.net/search?w=bookpage&book...  2005-03-07, 00:00:00  \n",
       "4  https://search.daum.net/search?w=bookpage&book...  2003-11-28, 00:00:00  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = main_func()\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 3. 본문 중 일부가 저장된 엑셀 파일을 바탕으로 전체 데이터를 크롤링하는 코드 \n",
    "> 위 main_func 함수를 실행하여 본문 중 일부가 포함된 데이터를 엑셀 파일로 저장해두고 이를 불러와야 합니다.<br><br>\n",
    "아래 3-3 ~ 3-6 을 확인하시면 실행 예시를 확인하실 수 있으며, <br>\n",
    "**위 2번 항목에서 저장한 파일을 crawl_full_contents 함수에게 전달**하면 <br>\n",
    "알아서 크롤링이 가능한 항목들을 전체 크롤링한 다음 **fullresult~~~ 이름의 엑셀 파일로 저장**해줍니다.<br>\n",
    "('web'은 각 사이트의 구조가 다르므로 일괄적인 웹크롤링이 불가능합니다.)<br><br>\n",
    "한번에 너무 많은 양의 문서를 크롤링하게되면 저장된 엑셀 파일이 열리지 않거나 DataFrame이 memory 관련 에러를 발생시킬 수 있습니다. <br>\n",
    "memory 관련 에러가 발생하면 다시 위 2번 항목으로 돌아가 보다 작은 페이지수를 입력하여 main_func 함수를 실행하고 <br>\n",
    "새롭게 저장된 엑셀파일을 활용해 전체 본문 크롤링을 실행해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3-1. URL 을 바탕으로 전체 본문을 크롤링하기 위한 함수 선언 (cafe, blog, tip, book 각각에 대하여)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_cafe(dataframe):\n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path='/Users/spark/Downloads/190806_Daum/chromedriver.exe') \n",
    "    # driver = webdriver.PhantomJS('(driver) phantomjs.exe')\n",
    "\n",
    "    full_contents = []\n",
    "    for index, url in enumerate(dataframe['urls']):    \n",
    "        \n",
    "        if index % 10 == 0:\n",
    "            print('{}번째 항목을 크롤링 중입니다 (URL : {})'.format(index, url))\n",
    "        \n",
    "        if url != 'no_url':\n",
    "            url = url[:7] + 'm.' + url[7:]\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            web_page = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            content = web_page.find('p', {'class':'cafe-editor-text'})\n",
    "            if content != None:\n",
    "                full_content = content.get_text()\n",
    "                full_content = full_content.replace(\"\\n\", \"\")\n",
    "                full_content = full_content.strip()\n",
    "            else:\n",
    "                full_content = '회원가입이 필요한 까페입니다.'\n",
    "        else:\n",
    "            full_content = '까페의 URL이 255자를 초과하여 엑셀파일에서 제외되었습니다.'\n",
    "            \n",
    "        full_contents.append(full_content)\n",
    "    \n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    \n",
    "    dataframe['full_contents'] = full_contents\n",
    "    print('\\nCrawling for full-contents is finished!')\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_blog(dataframe):\n",
    "    \n",
    "    full_contents = []\n",
    "    for index, url in enumerate(dataframe['urls']):    \n",
    "\n",
    "        if index % 10 == 0:\n",
    "            print('{}번째 항목을 크롤링 중입니다 (URL : {})'.format(index, url))\n",
    "\n",
    "        if 'tistory.com' in url:\n",
    "            div_class = 'tt_article_useless_p_margin'\n",
    "        elif 'brunch.co.kr' in url:\n",
    "            div_class = 'wrap_body'\n",
    "        elif 'blog.naver.com' in url:\n",
    "            div_class = 'se-main-container'\n",
    "        else:\n",
    "            div_class = None\n",
    "        \n",
    "        if div_class != None:\n",
    "            try:\n",
    "                response = urlopen(url)\n",
    "                source = BeautifulSoup(response, 'html.parser')\n",
    "                article = source.find('div', {'class':div_class})\n",
    "            except:\n",
    "                print('다음 항목 크롤링 중 빈번한 요청에 의해 한시적으로 요청이 막혔으나 나머지 항목을 대상으로 크롤링을 재개합니다. {}'.format(url))\n",
    "        else:\n",
    "            article = None\n",
    "\n",
    "        if article != None: \n",
    "            full_content = article.get_text()\n",
    "            full_content = full_content.replace(\"\\n\", \"\")\n",
    "            full_content = full_content.strip()\n",
    "        else:\n",
    "            full_content = '본문이 존재하지 않거나 크롤링이 불가능한 블로그 글입니다.'\n",
    "\n",
    "        full_contents.append(full_content)\n",
    "\n",
    "    dataframe['full_contents'] = full_contents\n",
    "    print('\\nCrawling for full-contents is finished!')\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_tip(dataframe):\n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path='/Users/spark/Downloads/190806_Daum/chromedriver.exe') \n",
    "    # driver = webdriver.PhantomJS('(driver) phantomjs.exe')\n",
    "\n",
    "    full_questions = []\n",
    "    full_answers = []\n",
    "    \n",
    "    for index, url in enumerate(dataframe['urls_question']):    \n",
    "        \n",
    "        if index % 10 == 0:\n",
    "            print('{}번째 항목을 크롤링 중입니다 (URL : {})'.format(index, url))\n",
    "        \n",
    "        url = url[:7] + 'm.' + url[7:]\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        web_page = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        full_question = web_page.find('div', {'class':'question_cont'}).find('div', {'class':'txt_collect'})\n",
    "        if full_question != None:\n",
    "            full_question = full_question.get_text()\n",
    "            full_question = full_question.replace(\"\\n\", \"\")\n",
    "            full_question = full_question.strip()\n",
    "        else:\n",
    "            full_question = '질문이 발견되지 않았습니다. (오픈지식에 해당합니다.)'\n",
    "\n",
    "        full_answer = web_page.find('div', {'id':'answerList'})\n",
    "        if full_answer != None:\n",
    "            full_answer = full_answer.find_all('div', {'class':'txt_collect'})\n",
    "            full_answer = ' '.join([answer.get_text().replace('\\n', '').strip() for answer in full_answer])\n",
    "        else:\n",
    "            full_answer = '답변이 발견되지 않았습니다.'\n",
    "\n",
    "        full_questions.append(full_question)\n",
    "        full_answers.append(full_answer)\n",
    "    \n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    \n",
    "    dataframe['full_questions'] = full_questions\n",
    "    dataframe['full_answers'] = full_answers\n",
    "    print('\\nCrawling for full-contents is finished!')\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_book(dataframe):\n",
    "\n",
    "    driver = webdriver.Chrome(executable_path='/Users/spark/Downloads/190806_Daum/chromedriver.exe') \n",
    "    # driver = webdriver.PhantomJS('(driver) phantomjs.exe')\n",
    "\n",
    "    full_bookinfos = []\n",
    "    for index, url in enumerate(dataframe['urls']):    \n",
    "\n",
    "        if index % 10 == 0:\n",
    "            print('{}번째 항목을 크롤링 중입니다 (URL : {})'.format(index, url))\n",
    "        \n",
    "        if url != 'no_url':\n",
    "            url = url[:8] + 'm.' + url[8:]\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            web_page = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            book_info_all = web_page.find_all('div', {'class':'info_desc'}) # 소개, 저자, 목차, 출판사서평\n",
    "\n",
    "            # 현재 코드에는 [소개, 저자, 목차, 출판사서평] 중 '소개'만 따내어 저장하도록 구현해두었습니다.\n",
    "            if book_info_all != None:\n",
    "                full_bookinfo = book_info_all[0].get_text()\n",
    "                full_bookinfo = full_bookinfo.replace(\"\\n\", \"\")\n",
    "                full_bookinfo = full_bookinfo.strip()\n",
    "            else:\n",
    "                full_bookinfo = '책의 정보가 발견되지 않았습니다.'\n",
    "        else:\n",
    "            full_bookinfo = '책의 정보가 발견되지 않았습니다.'\n",
    "            \n",
    "        full_bookinfos.append(full_bookinfo)\n",
    "\n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "\n",
    "    dataframe['full_bookinfos'] = full_bookinfos\n",
    "    print('\\nCrawling for full-contents is finished!')\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3-2. 위 2번 항목에서 저장한 [ 본문 일부 포함 ] 엑셀 파일을 받아들여 파일 이름에 따라 cafe/blog/tip/book 중 해당 항목 크롤러를 실행하는 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_full_contents(file_name):\n",
    "    \n",
    "    if 'web' in file_name:\n",
    "        print('[ web ]의 경우는 각 항목마다 웹사이트가 다르므로 본문 웹크롤링 적용이 불가능합니다.')\n",
    "        return False\n",
    "    \n",
    "    elif 'cafe' in file_name:\n",
    "        search_target = 'cafe'\n",
    "        original_df = pd.read_excel(file_name, encoding='utf-8')\n",
    "        original_df['urls'] = original_df['urls'].fillna('no_url')\n",
    "        full_df = full_cafe(original_df)\n",
    "        \n",
    "    elif 'blog' in file_name:\n",
    "        search_target = 'blog'\n",
    "        original_df = pd.read_excel(file_name, encoding='utf-8')\n",
    "        original_df['urls'] = original_df['urls'].fillna('no_url')\n",
    "        full_df = full_blog(original_df)\n",
    "        \n",
    "    elif 'tip' in file_name:\n",
    "        search_target = 'tip'\n",
    "        original_df = pd.read_excel(file_name, encoding='utf-8')\n",
    "        original_df['urls_question'] = original_df['urls_question'].fillna('no_url')\n",
    "        full_df = full_tip(original_df)\n",
    "        \n",
    "    elif 'book' in file_name:\n",
    "        search_target = 'book'\n",
    "        original_df = pd.read_excel(file_name, encoding='utf-8')\n",
    "        original_df['urls'] = original_df['urls'].fillna('no_url')\n",
    "        full_df = full_book(original_df)\n",
    "        \n",
    "    else:\n",
    "        print('\\n파일명을 확인해주세요! 파일 이름 안에 [ web / cafe / blog / tip / book ] 중 하나의 단어가 들어있어야 합니다.')\n",
    "        print('파일명 확인 후 다시 함수를 실행해주세요.')\n",
    "        return False\n",
    "\n",
    "    file_name = 'fullresult_{}_{}.xlsx'.format(search_target, datetime.datetime.now().strftime('%y%m%d_%H%M'))\n",
    "    full_df.to_excel(file_name, encoding='utf-8', index=False)\n",
    "    print('\\nFull-crawling result is saved at [ {} ]'.format(file_name))\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3-3. [본문 일부 포함] 엑셀 파일을 바탕으로 전체 본문 크롤링 및 엑셀파일 저장 예시 (cafe)\n",
    "> 회원가입 없이 바로 본문을 확인할 수 있는 까페 글들만 크롤링 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 항목을 크롤링 중입니다 (URL : http://blog.naver.com/PostView.nhn?blogId=autumnsk&logNo=221575363525)\n",
      "10번째 항목을 크롤링 중입니다 (URL : https://brunch.co.kr/@inmunart/636)\n",
      "20번째 항목을 크롤링 중입니다 (URL : https://brunch.co.kr/@yshinb/28)\n",
      "\n",
      "Crawling for full-contents is finished!\n",
      "\n",
      "Full-crawling result is saved at [ fullresult_blog_190806_2017.xlsx ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>contents</th>\n",
       "      <th>urls</th>\n",
       "      <th>datetimes</th>\n",
       "      <th>full_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>우리 아이 창의성 키우는 방법</td>\n",
       "      <td>아이들은 손을 문지르고, 얼굴도장을 찍기도 했다. 흥건해진 바닥 위로 폴짝폴짝 뛰다...</td>\n",
       "      <td>http://blog.naver.com/PostView.nhn?blogId=autu...</td>\n",
       "      <td>2019-07-01, 23:02:00</td>\n",
       "      <td>© jcao329, 출처 Unsplash 아이들을  초등학교 입학 전에  놀이 위주...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>창의성이라는 유령</td>\n",
       "      <td>꼰대가 되겠구나&amp;#39;, 하고요. 어린 민지에게 무례한 사람 대처법이, 제게는 &amp;...</td>\n",
       "      <td>http://free2world.tistory.com/1975</td>\n",
       "      <td>2019-02-27, 06:35:00</td>\n",
       "      <td>본문이 존재하지 않거나 크롤링이 불가능한 블로그 글입니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>철학과 창의성</td>\n",
       "      <td>지식보다는 창의성을 논하는 시대가 다가왔다. &amp;#39;Idea&amp;#39;가 문뜩 떠 ...</td>\n",
       "      <td>https://brunch.co.kr/@inmunart/402</td>\n",
       "      <td>2019-02-02, 17:45:00</td>\n",
       "      <td>지식보다는 창의성을 논하는 시대가 다가왔다.'Idea'가 문뜩 떠 올랐다고 말들을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&amp;#34;창의성&amp;#34;에 관하여</td>\n",
       "      <td>비슷한 경험이 있을 것이다. 그렇게 몰두하던 주제에 대한 해결책이 전혀 예기치 않던...</td>\n",
       "      <td>https://brunch.co.kr/@umcine/44</td>\n",
       "      <td>2018-12-16, 00:28:00</td>\n",
       "      <td>중요한 PT를 앞에 두고 주제를 어떻게 잡고 접근할지를 고민만 하는데 날짜는 다가오...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[성품 교육] 창의성에 대하여</td>\n",
       "      <td>손맛 따라 적당히 간 맞추듯 내 아이에 맞게, 내 걸음에 맞게 적당히 간 맞춰주세요...</td>\n",
       "      <td>http://yummystudy.tistory.com/426</td>\n",
       "      <td>2018-07-14, 13:41:00</td>\n",
       "      <td>본문이 존재하지 않거나 크롤링이 불가능한 블로그 글입니다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               titles                                           contents  \\\n",
       "0    우리 아이 창의성 키우는 방법  아이들은 손을 문지르고, 얼굴도장을 찍기도 했다. 흥건해진 바닥 위로 폴짝폴짝 뛰다...   \n",
       "1           창의성이라는 유령  꼰대가 되겠구나&#39;, 하고요. 어린 민지에게 무례한 사람 대처법이, 제게는 &...   \n",
       "2             철학과 창의성  지식보다는 창의성을 논하는 시대가 다가왔다. &#39;Idea&#39;가 문뜩 떠 ...   \n",
       "3  &#34;창의성&#34;에 관하여  비슷한 경험이 있을 것이다. 그렇게 몰두하던 주제에 대한 해결책이 전혀 예기치 않던...   \n",
       "4    [성품 교육] 창의성에 대하여  손맛 따라 적당히 간 맞추듯 내 아이에 맞게, 내 걸음에 맞게 적당히 간 맞춰주세요...   \n",
       "\n",
       "                                                urls             datetimes  \\\n",
       "0  http://blog.naver.com/PostView.nhn?blogId=autu...  2019-07-01, 23:02:00   \n",
       "1                 http://free2world.tistory.com/1975  2019-02-27, 06:35:00   \n",
       "2                 https://brunch.co.kr/@inmunart/402  2019-02-02, 17:45:00   \n",
       "3                    https://brunch.co.kr/@umcine/44  2018-12-16, 00:28:00   \n",
       "4                  http://yummystudy.tistory.com/426  2018-07-14, 13:41:00   \n",
       "\n",
       "                                       full_contents  \n",
       "0  © jcao329, 출처 Unsplash 아이들을  초등학교 입학 전에  놀이 위주...  \n",
       "1                   본문이 존재하지 않거나 크롤링이 불가능한 블로그 글입니다.  \n",
       "2  지식보다는 창의성을 논하는 시대가 다가왔다.'Idea'가 문뜩 떠 올랐다고 말들을 ...  \n",
       "3  중요한 PT를 앞에 두고 주제를 어떻게 잡고 접근할지를 고민만 하는데 날짜는 다가오...  \n",
       "4                   본문이 존재하지 않거나 크롤링이 불가능한 블로그 글입니다.  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_result = crawl_full_contents('result_blog_190806_1930_s.xlsx')\n",
    "full_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### etc. 다음 웹사이트 로그인이 필요할 경우에는 아래 코드를 활용하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4deb98ac2218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://logins.daum.net/accounts/loginform.do?mobilefull=1&category=cafe&url=http%3A%2F%2Fm.cafe.daum.net%2F_myCafe%3Fnull'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"//*[@id=\"mArticle\"]/div/div/div/div[2]/a[2]\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "driver.get('https://logins.daum.net/accounts/loginform.do?mobilefull=1&category=cafe&url=http%3A%2F%2Fm.cafe.daum.net%2F_myCafe%3Fnull') \n",
    "time.sleep(3) \n",
    "\n",
    "driver.find_element_by_xpath(\"\"\"//*[@id=\"mArticle\"]/div/div/div/div[2]/a[2]\"\"\").click()\n",
    "time.sleep(3) \n",
    "\n",
    "driver.find_element_by_xpath(\"\"\"//*[@id=\"id\"]\"\"\").send_keys('본인의 Daum 아이디') # id \n",
    "driver.find_element_by_xpath(\"\"\"//*[@id=\"inputPwd\"]\"\"\").send_keys('본인의 Daum 패스워드') # 패스워드 \n",
    "driver.find_element_by_xpath(\"\"\"//*[@id=\"loginBtn\"]\"\"\").click() # 입력 버튼 클릭 \n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
