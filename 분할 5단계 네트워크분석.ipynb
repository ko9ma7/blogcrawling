{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목적\n",
    "* 크롤링 결과로 네트워크분석\n",
    "* [참고](https://foreverhappiness.me/38)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선행설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T06:08:34.507495Z",
     "start_time": "2020-06-17T06:08:32.722350Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "import operator\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm_notebook  # 진행과정 시각화\n",
    "from datetime import timedelta  # 시간날짜\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import kss\n",
    "import re\n",
    "import matplotlib.font_manager as fm\n",
    "import gc\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "#한글깨짐방지\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<style>.container {width:100% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T06:08:34.518497Z",
     "start_time": "2020-06-17T06:08:34.508495Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def word_couple(df, year,stopword, custom_sw,keyword,verbose = False):\n",
    "    ''' 데이터프레임을 전달하면 단어쌍을 계산\n",
    "    명사에 한정해서 진행하며, 연도별로 나눌수 있게끔 진행\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    df(DataFrame) : 크롤링된 데이터프레임\n",
    "    year(int) : 분석할 연도\n",
    "    stopword(list) : 불용어\n",
    "    custom_sw(dictionary) : 단어별 개별 불용어\n",
    "    keyword(string) : 불용어처리를 위한 키워드\n",
    "    verbose(bool): 진행과정 확인\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    word_couple_df : DataFrame\n",
    "        word1    word2    freq\n",
    "    0   서핑     양양군   1187\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    import matplotlib.font_manager as fm\n",
    "    \n",
    "    #stopword\n",
    "    stopword = stopword +[keyword]\n",
    "    try :\n",
    "        stopword = stopword +custom_sw[keyword]\n",
    "        stopword = list(set(stopword))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    df = df.fillna(\"\")    \n",
    "    df['Noun'] = df['Noun'].apply(lambda x : re.sub(\"[\\[\\]' ]\",\"\",x).split(','))\n",
    "    \n",
    "    year_noun = df.loc[df.year == year,'Noun']\n",
    "    year_noun = year_noun.apply(lambda sentence : [word for word in sentence if word not in stopword])\n",
    "    corpus =list(map(lambda words : \" \".join(words),year_noun))\n",
    "    \n",
    "    #DTM\n",
    "    vector = CountVectorizer()\n",
    "    vector.fit(corpus) #코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
    "    values = vector.transform(corpus).toarray()\n",
    "    cols = vector.get_feature_names() # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.\n",
    "    DTM_df = pd.DataFrame(values,columns= cols)\n",
    "    \n",
    "    # 단어의수가 70~80개가 되는 적정범위 산출\n",
    "    \n",
    "    imsi = DTM_df.sum()\n",
    "    count = 0\n",
    "    fin = 0\n",
    "    start = 0\n",
    "    end = imsi.max()\n",
    "    min_v = 70\n",
    "    max_v = 80\n",
    "    while (fin>max_v)|(fin<min_v):\n",
    "        y = random.randint(start, end)\n",
    "        fin = imsi[imsi>y].shape[0]\n",
    "\n",
    "        if fin <= min_v:\n",
    "            end = y\n",
    "        elif fin >= max_v:\n",
    "            start = y\n",
    "            \n",
    "        if count> 15:\n",
    "            break\n",
    "    count_min = y\n",
    "\n",
    "    select_cols = imsi[imsi>count_min].index.values\n",
    "    DTM_df = DTM_df[select_cols]\n",
    "    cols = DTM_df.columns.values\n",
    "    word_length = len(cols)\n",
    "    \n",
    "    #단어쌍 계산\n",
    "    count_dict = {}\n",
    "    for i in range(len(DTM_df)): #각 블로그글 \n",
    "        sample = DTM_df.T.iloc[:,i]\n",
    "        sample = sample[sample>0]\n",
    "        len_max = len(sample)\n",
    "        for j in range(len_max):\n",
    "            for z in range(j+1,len_max):\n",
    "                count_dict[sample.index[j], sample.index[z]] = count_dict.get((sample.index[j], sample.index[z]), 0) + max(sample[sample.index[j]], sample[sample.index[z]])\n",
    "\n",
    "    count_list = []\n",
    "    for words in count_dict:\n",
    "        count_list.append([words[0],words[1],count_dict[words]])\n",
    "    word_couple_df = pd.DataFrame(count_list, columns = ['word1','word2','freq'])\n",
    "    word_couple_df = word_couple_df.sort_values(by='freq',ascending= False)\n",
    "    word_couple_df.reset_index(drop =True,inplace = True)\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(keyword, year)\n",
    "        print(\"연도별 대상 row수:\",year_noun.shape[0])\n",
    "        print(\"count_min:\", count_min)\n",
    "        print(\"단어 길이:\", word_length)\n",
    "        print(\"word_couple_df 길이:\",word_couple_df.shape[0])\n",
    "        print(\"------------------\")\n",
    "    \n",
    "    return word_couple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T06:08:34.529500Z",
     "start_time": "2020-06-17T06:08:34.520497Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def networkx_graph(df, keyword, year, font_size = 18, k = 7, save=True):\n",
    "    '''데이터프레임을 전달하면 네트워크분석 진행\n",
    "    word_couple함수를 통해 나온 결과를 전달하면, 네트워크분석결과를 전달\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    df(DataFrame) : word_couple함수의 결과인 데이터프레임 전송\n",
    "    keyword(string) : 파일저장시 사용할 keyword\n",
    "    year(int) : 파일저장시 사용할 year\n",
    "    min_cuple(int) : 최소 frq의 크기 (단어의 개수를 결정)\n",
    "    font_size(int) : 출력되는 폰트의 크기\n",
    "    k(int) : 노드간의 거리 조절(클수록 거리가 멀어짐)\n",
    "    save(bool) : 저장 결정여부 \n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    G : network 출력 결과 \n",
    "    '''\n",
    "    import matplotlib.font_manager as fm\n",
    "    \n",
    "    # 적정 min_couple 70~80산출\n",
    "    count = 0\n",
    "    fin = 0\n",
    "    start = 0\n",
    "    end = df.freq.max()\n",
    "    min_v = 70\n",
    "    max_v = 80\n",
    "\n",
    "    while (fin >max_v)|(fin<min_v):\n",
    "        y = random.randint(start, end)\n",
    "        fin = df[df.freq>y].shape[0]\n",
    "        \n",
    "        count +=1\n",
    "        if fin <= min_v:\n",
    "            end = y\n",
    "        elif fin >= max_v:\n",
    "            start = y\n",
    "            \n",
    "        if count >15:\n",
    "            break\n",
    "    min_couple = y\n",
    "    \n",
    "    #network\n",
    "    G_centrality = nx.Graph() #중심성척도계산을 위한 그래프\n",
    "    for ind in range((len(np.where(df['freq'] >=min_couple)[0]))):\n",
    "        G_centrality.add_edge(df['word1'][ind],df['word2'][ind], weight = int(df['freq'][ind])) #word1,word2간의 빈도수를 갖는 edge생성 \n",
    "    dgr = nx.degree_centrality(G_centrality) #연결중심성(직접적으로 연결된 노드의수 )\n",
    "   \n",
    "    #중심성이 큰순으로 정렬 \n",
    "    sorted_dgr = sorted(dgr.items(),key=operator.itemgetter(1), reverse = True)\n",
    "  \n",
    "    #네트워크 그릴 그래프 선언\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    #페이지 랭크에 따라 두 노드사이 연관성 결정\n",
    "    for i in range(len(sorted_dgr)):\n",
    "        G.add_node(sorted_dgr[i][0], nodesize = sorted_dgr[i][1])\n",
    "    for ind in range((len(np.where(df['freq']>=min_couple)[0]))):\n",
    "        G.add_weighted_edges_from([(df['word1'][ind],df['word2'][ind],int(df['freq'][ind]))])\n",
    "        \n",
    "    #노드크기\n",
    "    sizes = [G.nodes[node]['nodesize']*700 for node in G]\n",
    "    \n",
    "    #컬러, 라벨다는 것 등 \n",
    "    options = {\n",
    "        'edge_color' : '#FFDEA2',\n",
    "        'node_color' : '#F09B7B',\n",
    "        'width':1,\n",
    "        'with_labels':True,\n",
    "        'font_weight':'regular'\n",
    "    }\n",
    "    #한글 설정\n",
    "    fm._rebuild()\n",
    "    font_fname = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "    fontprop = fm.FontProperties(fname=font_fname, size = 20).get_name()\n",
    "    \n",
    "    #그래프 생성\n",
    "    plt.figure(figsize= (20,10))\n",
    "    plt.title(f'{year}',fontsize = 20) \n",
    "    nx.draw(G,node_size=sizes, pos = nx.spring_layout(G, k=5.5, iterations= 100), **options, font_family=fontprop, font_size=font_size)\n",
    "    ax = plt.gca()\n",
    "    ax.collections[0].set_edgecolor(\"#555555\")\n",
    "    \n",
    "    if save == True:\n",
    "        os.makedirs(f\"./output/네트워크분석/{keyword}\",exist_ok=True)\n",
    "        plt.savefig(f\"./output/네트워크분석/{keyword}/{keyword}_{year}_networkx.png\",bbox_inches='tight')\n",
    "    plt.close()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T06:08:34.556515Z",
     "start_time": "2020-06-17T06:08:34.530499Z"
    }
   },
   "outputs": [],
   "source": [
    "#stopword\n",
    "sw = list(pd.read_excel(\"stopword(cp949).xlsx\",encoding = 'cp949')['불용어']) #불용어 불러오기\n",
    "\n",
    "#개별 불용어 \n",
    "with open('custom_sw.json') as load_file:\n",
    "    custom_sw = json.load(load_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T06:12:19.786820Z",
     "start_time": "2020-06-17T06:08:34.557515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2329ded95fc4fc38f0ce7b41726c287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = './output/token_통합/'\n",
    "file_list = os.listdir(path)\n",
    "for file in tqdm_notebook(file_list):\n",
    "    df = pd.read_csv(path+file)\n",
    "    keyword = file.split('_')[0]\n",
    "    for year in range(2011, 2020,2): #간격조정 가능\n",
    "        word_couple_df = word_couple(df,year,sw, custom_sw,keyword)\n",
    "        G = networkx_graph(word_couple_df,keyword, year,font_size = 18, k= 7,save = True)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T06:12:19.801354Z",
     "start_time": "2020-06-17T06:12:19.786820Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_gif(path):\n",
    "    '''\n",
    "    이미지 여러장이 들어있는 폴더를 input하면 gif를 만들어냄\n",
    "    jpg, png파일만 허용\n",
    "    '''\n",
    "    from PIL import Image\n",
    "    import os\n",
    "    import imageio\n",
    "    \n",
    "    file_list = os.listdir(path)\n",
    "    \n",
    "    #select png\n",
    "    png_ls =[]\n",
    "    for file in file_list:\n",
    "        try :\n",
    "            if file.split('.')[1] in (['png','jpg']):\n",
    "                png_ls.append(file)\n",
    "        except : \n",
    "            pass\n",
    "    \n",
    "    #naming\n",
    "    keyword = png_ls[0].split('_')[0]\n",
    "    start = png_ls[0].split('_')[1]\n",
    "    end = png_ls[-1].split('_')[1]\n",
    "                                      \n",
    "    images = [np.array(Image.open(path+file)) for file in png_ls]\n",
    "    imageio.mimsave(f'./output/네트워크분석/{keyword}_{start}~{end}.gif', images, fps=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T06:12:22.681238Z",
     "start_time": "2020-06-17T06:12:19.802259Z"
    }
   },
   "outputs": [],
   "source": [
    "path = './output/네트워크분석'\n",
    "\n",
    "for folder in os.listdir(path):\n",
    "    if os.path.splitext(folder)[1] == \"\":\n",
    "        make_gif(f'{path}/{folder}/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 466,
   "position": {
    "height": "40px",
    "left": "1187px",
    "right": "20px",
    "top": "111px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
