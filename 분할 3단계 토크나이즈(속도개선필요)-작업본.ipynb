{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# issue\n",
    "* tokenize 작업의 속도가 굉장히 느림 개선방법이 있다면 고민해봐야함 (작업내용 1차적으로 보존)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T01:10:38.457964Z",
     "start_time": "2020-06-05T01:10:35.648357Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:217: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm,tqdm_notebook  # 진행과정 시각화\n",
    "tqdm.pandas() #apply사용\n",
    "from datetime import timedelta  # 시간날짜\n",
    "from apyori import apriori  # 연관분석\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup  # 크롤링\n",
    "\n",
    "import wordcloud\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Komoran,Okt,Kkma,Twitter  # 자연어처리\n",
    "# from eunjeon import Mecab #은전한닢\n",
    "okt = Okt(max_heap_size=5120)\n",
    "# mecab = Mecab()\n",
    "komoran = Komoran(userdic='user_dictionary.txt')\n",
    "\n",
    "#한글깨짐방지\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<style>.container {width:100% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:10:21.534315Z",
     "start_time": "2020-06-01T04:10:21.529314Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tokenized(dataframe, file_name, keyword, stopword):\n",
    "    '''\n",
    "    okt : 형태소분석기 무엇사용?\n",
    "    dataframe : 분석할 df\n",
    "    keyword : sw에 추가하기 위함\n",
    "    '''\n",
    "\n",
    "    # stopword에 검색어 추가\n",
    "    stopword.append(keyword.split(' ')[0])\n",
    "    \n",
    "    start = 0\n",
    "    end = len(dataframe)\n",
    "    \n",
    "    print(keyword)\n",
    "    if end<=20000:    \n",
    "        # tqdm.pandas(desc = '토큰화')\n",
    "        token_df = dataframe.iloc[start:end]['full_text'].progress_apply(lambda x: okt.pos(x, norm=True, stem=True))\n",
    "        token_df_e = token_df.explode()\n",
    "        # tqdm.pandas(desc = '단어추출')\n",
    "        token_noun = token_df_e.apply(lambda x: x[0] if x[1] == 'Noun' else np.nan).dropna()\n",
    "        token_adj = token_df_e.apply(lambda x: x[0] if x[1] == 'Adjective' else np.nan).dropna()\n",
    "\n",
    "        # 클리닝\n",
    "        # tqdm.pandas(desc = '클리닝')\n",
    "        token_noun = token_noun[token_noun.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "        token_adj = token_adj[token_adj.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "        \n",
    "        #저장\n",
    "        concated_df = pd.concat([token_noun,token_adj],axis = 1)\n",
    "        concated_df.to_csv('D:/Python/블로그크롤링/output/token/{keyword}_{start}~{end}.csv'.format(keyword = keyword, start = start, end = end))\n",
    "#         # 저장\n",
    "# #         defalut = pd.Series(index=range(start, end), data=range(start, end), name='index')\n",
    "#         dataframe = pd.merge(dataframe, token_noun.groupby(level=0).agg(list).rename(\n",
    "#             'Noun'), how='left', left_index=True, right_index=True)\n",
    "#         dataframe = pd.merge(dataframe, token_adj.groupby(level=0).agg(list).rename(\n",
    "#             'Adjective'), how='left', left_index=True, right_index=True)\n",
    "#         pd.DataFrame(dataframe).to_csv('D:/Python/블로그크롤링/output/token/{keyword}_{start}~{end}.csv'.format(keyword = keyword, start = start, end = end))\n",
    "        \n",
    "    else : #20000개 이상인경우 분할 작업\n",
    "        final_end = len(dataframe)\n",
    "        epoch = (final_end//20000) +1\n",
    "        start = 0\n",
    "        end = 20000\n",
    "        while epoch != 0:\n",
    "            # tqdm.pandas(desc = '토큰화')\n",
    "            token_df = dataframe.iloc[start:end]['full_text'].progress_apply(lambda x: okt.pos(x, norm=True, stem=True))\n",
    "            token_df_e = token_df.explode()\n",
    "            # tqdm.pandas(desc = '단어추출')\n",
    "            token_noun = token_df_e.apply(lambda x: x[0] if x[1] == 'Noun' else np.nan).dropna()\n",
    "            token_adj = token_df_e.apply(lambda x: x[0] if x[1] == 'Adjective' else np.nan).dropna()\n",
    "\n",
    "            # 클리닝\n",
    "            # tqdm.pandas(desc = '클리닝')\n",
    "            token_noun = token_noun[token_noun.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "            token_adj = token_adj[token_adj.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "\n",
    "            #저장\n",
    "            concated_df = pd.concat([token_noun,token_adj],axis = 1)\n",
    "            os.makedirs(\n",
    "            './output/token/{file_name}'.format(file_name=file_name), exist_ok=True)\n",
    "            concated_df.to_csv('D:/Python/블로그크롤링/output/token/{keyword}_{start}~{end}.csv'.format(keyword = keyword, start = start, end = end))\n",
    "            \n",
    "#             # defalut = pd.Series(index=range(start, end), data=range(start, end), name='index')\n",
    "#             dataframe = pd.merge(dataframe, token_noun.groupby(level=0).agg(list).rename(\n",
    "#                 'Noun'), how='left', left_index=True, right_index=True)\n",
    "#             dataframe = pd.merge(dataframe, token_adj.groupby(level=0).agg(list).rename(\n",
    "#                 'Adjective'), how='left', left_index=True, right_index=True)\n",
    "            \n",
    "#             os.makedirs(\n",
    "#             './output/token/{file_name}'.format(file_name=file_name), exist_ok=True)\n",
    "#             pd.DataFrame(dataframe).to_csv('D:/Python/블로그크롤링/output/token/{file_name}/{file_name}_{start}~{end}.csv'.format(file_name = file_name, start = start, end = end))\n",
    "            \n",
    "            #next turn\n",
    "            if epoch >2 :\n",
    "                start += 20000\n",
    "                end += 20000\n",
    "            else :      \n",
    "                start += 20000\n",
    "                end = final_end\n",
    "\n",
    "            tokens_dict = dict()\n",
    "            tokens_dict['Nouns'] = []\n",
    "            tokens_dict['Adj'] = []\n",
    "            epoch -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:10:21.538316Z",
     "start_time": "2020-06-01T04:10:21.535315Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sw = list(pd.read_csv(\"stopword.csv\")['불용어']) #불용어 불러오기\n",
    "# path = 'D:/Python/블로그크롤링/output/크롤링/통합/'\n",
    "# file_list = os.listdir(path)\n",
    "# for file in file_list:\n",
    "#     main_df = pd.read_csv(path + file)\n",
    "#     keyword = file_list[0].split('_')[0]\n",
    "#     tokenized(okt,main_df,keyword,sw)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T01:21:13.069934Z",
     "start_time": "2020-06-05T01:21:12.533814Z"
    }
   },
   "outputs": [],
   "source": [
    "sw = list(pd.read_csv(\"stopword.csv\")['불용어']) #불용어 불러오기\n",
    "path = 'D:/Python/블로그크롤링/output/크롤링/통합/'\n",
    "file_list = os.listdir(path)\n",
    "file = '광진 +양양_통합_5044.csv'\n",
    "main_df = pd.read_csv(path + file)\n",
    "keyword = '양양'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 방법2\n",
    "tokenize을 일부부분을 apply로 진행, 10000개 10분, 메모리 에러문제로 그이상 확인불가  \n",
    "tqdm.pandas()로 apply 진행상황확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T05:47:12.100491Z",
     "start_time": "2020-06-01T05:47:12.096491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85670"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T08:24:11.298491Z",
     "start_time": "2020-06-01T08:05:34.165743Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 15670/15670 [18:21<00:00, 14.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# 저장할 위치\n",
    "tokens_dict = dict()\n",
    "tokens_dict['Nouns'] = []\n",
    "tokens_dict['Adj'] = []\n",
    "\n",
    "start = 70000\n",
    "end = 85670\n",
    "# tqdm.pandas(desc = '토큰화')\n",
    "a = main_df.iloc[start:end]['full_text'].progress_apply(lambda x: okt.pos(x, norm=True, stem=True))\n",
    "a_e = a.explode()\n",
    "# tqdm.pandas(desc = '단어추출')\n",
    "a_e_n = a_e.apply(lambda x: x[0] if x[1] == 'Noun' else np.nan).dropna()\n",
    "a_e_ad = a_e.apply(lambda x: x[0] if x[1] == 'Adjective' else np.nan).dropna()\n",
    "\n",
    "# 클리닝\n",
    "# tqdm.pandas(desc = '클리닝')\n",
    "a_e_n = a_e_n[a_e_n.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "a_e_ad = a_e_ad[a_e_ad.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "\n",
    "\n",
    "# 저장\n",
    "defalut = pd.Series(index=range(start, end), data=range(start, end), name='index')\n",
    "defalut = pd.merge(defalut, a_e_n.groupby(level=0).agg(list).rename(\n",
    "    'Noun'), how='left', left_index=True, right_index=True)\n",
    "defalut = pd.merge(defalut, a_e_ad.groupby(level=0).agg(list).rename(\n",
    "    'Adjective'), how='left', left_index=True, right_index=True)\n",
    "pd.DataFrame(defalut).to_csv('D:/Python/블로그크롤링/output/token/강원도_{start}~{end}.csv'.format(start = start, end = end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 속도가 느려서 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T01:23:37.327597Z",
     "start_time": "2020-06-05T01:21:15.188413Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add0585cf44b4fa9b3c73e1df72eedba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='양양tokenize', max=500.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ori \n",
    "#저장할 위치\n",
    "tokens_dict = dict()\n",
    "tokens_dict['Nouns'] = []\n",
    "tokens_dict['Adj'] = []\n",
    "for blog_num in tqdm_notebook(range(0,len(main_df[:500])),desc = keyword+'tokenize'):\n",
    "    #블로그 품사태깅 (load한 경우 list가 아닌 text로 받아서 첫번째가 아닌 전체를 받는다)\n",
    "#     pos_text = okt.pos(target[blog_num][0], norm=True, stem=True)\n",
    "        pos_text = okt.pos(main_df['full_text'][blog_num], norm=True, stem=True)\n",
    "\n",
    "        pos_text_df = pd.DataFrame(pos_text)\n",
    "        noun_lists = pos_text_df[pos_text_df[1].apply(lambda x : (x in \"Noun\"))][0].values\n",
    "        adj_lists = pos_text_df[pos_text_df[1].apply(lambda x : (x in \"Adjective\"))][0].values\n",
    "        #클리닝\n",
    "        clean_nouns = list(filter(lambda word : (len(word)>1)&(word not in sw),noun_lists))\n",
    "        clean_adjs = list(filter(lambda word : (len(word)>1)&(word not in sw),adj_lists))\n",
    "        #dictionay 저장\n",
    "        tokens_dict.get('Nouns',0).append(clean_nouns)\n",
    "        tokens_dict.get('Adj',0).append(clean_adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:10:06.412632Z",
     "start_time": "2020-06-01T04:09:49.363Z"
    }
   },
   "outputs": [],
   "source": [
    "#tunning 1\n",
    "#저장할 위치\n",
    "tokens_dict = dict()\n",
    "tokens_dict['Nouns'] = []\n",
    "tokens_dict['Adj'] = []\n",
    "\n",
    "for txt in tqdm_notebook(main_df['full_text'].values[:500] ,desc = keyword+'tokenize'):\n",
    "    a = okt.pos(txt, norm=True, stem=True)\n",
    "    noun_lists = []\n",
    "    adj_lists = []\n",
    "    for word, dtype in a:\n",
    "        if dtype == 'Noun':\n",
    "            noun_lists.append(word)\n",
    "        elif dtype == 'Adjective':\n",
    "            adj_lists.append(word)\n",
    "    #클리닝\n",
    "    clean_nouns = list(filter(lambda word : (len(word)>1)&(word not in sw),noun_lists))\n",
    "    clean_adjs = list(filter(lambda word : (len(word)>1)&(word not in sw),adj_lists))\n",
    "    #dictionay 저장\n",
    "    tokens_dict.get('Nouns',0).append(clean_nouns)\n",
    "    tokens_dict.get('Adj',0).append(clean_adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:10:06.412632Z",
     "start_time": "2020-06-01T04:09:49.364Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tunning 2 : DF.isin\n",
    "#저장할 위치\n",
    "tokens_dict = dict()\n",
    "tokens_dict['Nouns'] = []\n",
    "tokens_dict['Adj'] = []\n",
    "num = 1\n",
    "count = 1\n",
    "for txt in tqdm_notebook(main_df['full_text'].values[:500] ,desc = keyword+'tokenize'):\n",
    "    \n",
    "    a = okt.pos(txt, norm=True, stem=True)\n",
    "\n",
    "    a_df = pd.DataFrame(a)\n",
    "    noun_lists = a_df[a_df[1].isin(['Noun'])][0].values\n",
    "    adj_lists = a_df[a_df[1].isin(['Adjective'])][0].values\n",
    "    #클리닝\n",
    "    clean_nouns = list(filter(lambda word : (len(word)>1)&(word not in sw),noun_lists))\n",
    "    clean_adjs = list(filter(lambda word : (len(word)>1)&(word not in sw),adj_lists))\n",
    "    #dictionay 저장\n",
    "    tokens_dict.get('Nouns',0).append(clean_nouns)\n",
    "    tokens_dict.get('Adj',0).append(clean_adjs)\n",
    "    \n",
    "    gc.collect()\n",
    "    num += 1\n",
    "    \n",
    "    if num == 100:\n",
    "        #중간저장 \n",
    "        tokens_df = pd.DataFrame(tokens_dict)\n",
    "        tokens_df.to_csv('D:/Python/블로그크롤링/output/token/{keyword}_token_{count}.csv'.format(keyword = keyword,count = count)) #token을 원래문장과 비교하려면 동일한 인덱스로 추정하면됨\n",
    "        count += 1\n",
    "        num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:10:06.413632Z",
     "start_time": "2020-06-01T04:09:49.365Z"
    }
   },
   "outputs": [],
   "source": [
    "#tunning 3 : np.isin\n",
    "#저장할 위치\n",
    "tokens_dict = dict()\n",
    "tokens_dict['Nouns'] = []\n",
    "tokens_dict['Adj'] = []\n",
    "#append 미리선언\n",
    "add_noun = tokens_dict.get('Nouns',0).append\n",
    "add_adj = tokens_dict.get('Adj',0).append\n",
    "for txt in tqdm_notebook(main_df['full_text'].values[:500] ,desc = keyword+'tokenize'):\n",
    "    tagged = okt.pos(txt, norm=True, stem=True)\n",
    "    \n",
    "    #array로 변환후 작업\n",
    "    tagged_arr= np.array(tagged)\n",
    "\n",
    "#     n_mask = np.isin(tagged_arr[:,1],\"Noun\")\n",
    "#     a_mask = np.isin(tagged_arr[:,1],\"Adjective\")\n",
    "\n",
    "    noun_lists = tagged_arr[np.isin(tagged_arr[:,1],\"Noun\")][:,0]\n",
    "    adj_lists = tagged_arr[np.isin(tagged_arr[:,1],\"Adjective\")][:,0]\n",
    "    #클리닝\n",
    "    clean_nouns = list(filter(lambda word : (len(word)>1)&(word not in sw),noun_lists))\n",
    "    clean_adjs = list(filter(lambda word : (len(word)>1)&(word not in sw),adj_lists))\n",
    "    #dictionay 저장\n",
    "    add_noun(clean_nouns)\n",
    "    add_adj(clean_adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
