{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# issue\n",
    "* 사용자사전을 위해 ckonlpy 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T05:36:01.611655Z",
     "start_time": "2021-01-12T05:36:01.400607Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm,tqdm_notebook  # 진행과정 시각화\n",
    "tqdm.pandas() #apply사용\n",
    "from datetime import timedelta  # 시간날짜\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import math \n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Komoran,Okt,Kkma,Twitter  # 자연어처리\n",
    "okt = Okt(max_heap_size=5120)\n",
    "\n",
    "import ckonlpy\n",
    "tw = ckonlpy.tag.Twitter()\n",
    "new_noun = pd.read_excel('단어사전.xlsx')['단어'].to_list()\n",
    "tw.add_dictionary(new_noun,'Noun')\n",
    "\n",
    "#한글깨짐방지\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<style>.container {width:100% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T05:36:27.249810Z",
     "start_time": "2021-01-12T05:36:27.239808Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tokenized(path, dataframe, keyword, stopword, batch_size = 10000):\n",
    "    '''크롤링된 데이터를 불러와 ckonlpy로 토크나이즈 진행\n",
    "    사용자정의사전을 쉽게 등록할 수 있어 ckonlpy를 사용하여 진행\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    path(string) : 저장경로\n",
    "    dataframe(DataFrame) : 분석할 df\n",
    "    keyword(string) : sw에 추가하기 위함\n",
    "    stopword(list) : 불용어 리스트\n",
    "    batch_size : 한번에 진행할 row수 (과도하게 늘리면 Java heap memory오류 발생)\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    DataFrame 형태로 './output/token/{keyword}_{start}~{end}.csv' 형태로 저장\n",
    "    post_dates   year   month  title  text     url    Noun        Adjective\n",
    "    2011-02-31   2011   02     제목   [전체내용]    url    [명사군]    [형용사군]\n",
    "\n",
    "    '''\n",
    "    #저장 위치 \n",
    "    os.makedirs(f'./{path}/token/{keyword}',exist_ok=True)\n",
    "    # stopword에 검색어 추가\n",
    "    stopword.append(keyword.split(' ')[0])\n",
    "    \n",
    "    start = 0\n",
    "    end = len(dataframe)\n",
    "    \n",
    "    print(keyword)\n",
    "    if end<=batch_size:    \n",
    "        # tqdm.pandas(desc = '토큰화')\n",
    "        token_df = dataframe.iloc[start:end]['full_text'].progress_apply(lambda x: tw.pos(x, norm=True, stem=True))\n",
    "        \n",
    "        token_df_e = token_df.explode()\n",
    "        # tqdm.pandas(desc = '단어추출')\n",
    "        token_noun = token_df_e.apply(lambda x: x[0] if x[1] == 'Noun' else np.nan).dropna()\n",
    "        token_adj = token_df_e.apply(lambda x: x[0] if x[1] == 'Adjective' else np.nan).dropna()\n",
    "\n",
    "        # 클리닝\n",
    "        # tqdm.pandas(desc = '클리닝')\n",
    "        token_noun = token_noun[token_noun.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "        token_adj = token_adj[token_adj.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "        \n",
    "        #저장\n",
    "        main_df = pd.merge(dataframe, token_noun.groupby(level=0).agg(list).rename(\n",
    "            'Noun'), how='left', left_index=True, right_index=True)\n",
    "        main_df = pd.merge(main_df, token_adj.groupby(level=0).agg(list).rename(\n",
    "            'Adjective'), how='left', left_index=True, right_index=True)\n",
    "        main_df.to_csv(f'./{path}/token/{keyword}_{start}~{end}.csv')\n",
    "        \n",
    "    else : #10000개 이상인경우 분할 작업\n",
    "        final_end = len(dataframe)\n",
    "        epoch = math.ceil(final_end/batch_size)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while epoch != 0:\n",
    "            # tqdm.pandas(desc = '토큰화')\n",
    "            token_df = dataframe.iloc[start:end]['full_text'].progress_apply(lambda x: tw.pos(x, norm=True, stem=True))\n",
    "            token_df_e = token_df.explode()\n",
    "            # tqdm.pandas(desc = '단어추출')\n",
    "            token_noun = token_df_e.apply(lambda x: x[0] if x[1] == 'Noun' else np.nan).dropna()\n",
    "            token_adj = token_df_e.apply(lambda x: x[0] if x[1] == 'Adjective' else np.nan).dropna()\n",
    "\n",
    "            # 클리닝\n",
    "            # tqdm.pandas(desc = '클리닝')\n",
    "            token_noun = token_noun[token_noun.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "            token_adj = token_adj[token_adj.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "\n",
    "            #저장\n",
    "            main_df = pd.merge(dataframe.iloc[start:end], token_noun.groupby(level=0).agg(list).rename(\n",
    "                'Noun'), how='left', left_index=True, right_index=True)\n",
    "            main_df = pd.merge(main_df, token_adj.groupby(level=0).agg(list).rename(\n",
    "                'Adjective'), how='left', left_index=True, right_index=True)\n",
    "#             os.makedirs(f'./new_output/token/{keyword}',exist_ok=True)\n",
    "            main_df.to_csv(f'./{path}/token/{keyword}/{keyword}_{start}~{end}.csv')\n",
    "                   \n",
    "            #next turn\n",
    "            if epoch >2 :\n",
    "                start += batch_size\n",
    "                end += batch_size\n",
    "            else :      \n",
    "                start += batch_size\n",
    "                end = final_end\n",
    "            \n",
    "            epoch -= 1\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### heap 메모리 부족 오류 발생\n",
    "* heap메모리를 4GB부여하고, 15000개 단위로 분할하여 작업진행 (2만은 가끔 메모리터짐)\n",
    "* 저장시 기본사항을 붙일수 있게 작업\n",
    "* 진행이 길어질수록 속도감소(메모리문제로 추정)하기 때문에 재실행필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T05:39:02.667586Z",
     "start_time": "2021-01-12T05:36:28.650063Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549c6478edc146129b7d4ab665c23a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 5/6277 [00:00<02:58, 35.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강릉\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6277/6277 [02:29<00:00, 42.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#임시 속초/속초해수욕장\n",
    "sw = list(pd.read_excel(\"stopword(cp949).xlsx\",encoding = 'cp949')['불용어']) #불용어 불러오기\n",
    "path = 'new_output'\n",
    "folder_path = f'./{path}/크롤링_통합/'\n",
    "file_list = os.listdir(folder_path)\n",
    "for file in tqdm_notebook(file_list[:31]):\n",
    "    file_name = file.split('.')[0]\n",
    "    main_df = pd.read_csv(folder_path + file)\n",
    "    keyword = file.split('_')[0]\n",
    "    tokenized(path, main_df, keyword, sw)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T04:27:09.880714Z",
     "start_time": "2020-06-15T04:27:09.876714Z"
    }
   },
   "outputs": [],
   "source": [
    "# file = '광진 +양양_통합_5044.csv'\n",
    "# file_name = file.split('.')[0]\n",
    "# main_df = pd.read_csv(path + file)\n",
    "# keyword = file.split('_')[0]\n",
    "# tokenized(main_df,file_name,keyword,sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T04:27:09.885716Z",
     "start_time": "2020-06-15T04:27:09.881715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['강릉 +강원도_통합_53544.csv',\n",
       " '강원도_통합_61075.csv',\n",
       " '광진 +양양_통합_460.csv',\n",
       " '광진리 +강원도_통합_398.csv',\n",
       " '광진해변 +양양_통합_292.csv',\n",
       " '광진해수욕장 +양양_통합_194.csv',\n",
       " '기사문 +강원도_통합_990.csv',\n",
       " '기사문 +양양_통합_1760.csv',\n",
       " '기사문항 +양양_통합_876.csv',\n",
       " '기사문해변 +양양_통합_1044.csv',\n",
       " '낙산 +강원도_통합_12616.csv',\n",
       " '낙산 +양양_통합_14908.csv',\n",
       " '낙산항 +양양_통합_336.csv',\n",
       " '낙산해수욕장_통합_8083.csv',\n",
       " '남애 +양양_통합_2079.csv',\n",
       " '남애항 +양양_통합_3130.csv',\n",
       " '동산 +양양_통합_1888.csv',\n",
       " '동산항 +양양_통합_464.csv',\n",
       " '동산해수욕장 +양양_통합_567.csv',\n",
       " '동호 +강원도_통합_2523.csv',\n",
       " '동호 +양양_통합_2486.csv',\n",
       " '동호해변 +양양_통합_1881.csv',\n",
       " '동호해수욕장 +양양_통합_725.csv',\n",
       " '물치 +강원도_통합_1250.csv',\n",
       " '물치 +양양_통합_1852.csv',\n",
       " '물치리 +강원도_통합_255.csv',\n",
       " '물치항 +양양_통합_2660.csv',\n",
       " '설악해수욕장 +강원도_통합_939.csv',\n",
       " '설악해수욕장 +양양_통합_1235.csv',\n",
       " '속초 +강원도_통합_57030.csv',\n",
       " '수산항 +양양_통합_2599.csv',\n",
       " '양양 +강원도_통합_47651.csv',\n",
       " '오산 +양양_통합_2028.csv',\n",
       " '오산리 +양양_통합_1585.csv',\n",
       " '오산해수욕장 +양양_통합_842.csv',\n",
       " '인구리 +강원도_통합_488.csv',\n",
       " '전진리 +강원도_통합_539.csv',\n",
       " '죽도 +양양_통합_8355.csv',\n",
       " '하광정리 +강원도_통합_634.csv',\n",
       " '하조대 +강원도_통합_11099.csv',\n",
       " '하조대 +양양_통합_13866.csv',\n",
       " '하조대해수욕장 +양양_통합_5836.csv',\n",
       " '후진항 +양양_통합_572.csv',\n",
       " '휴휴암 +양양_통합_5511.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
