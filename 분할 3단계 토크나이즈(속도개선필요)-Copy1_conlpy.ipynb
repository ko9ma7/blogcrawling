{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# issue\n",
    "* tokenize 작업의 속도가 굉장히 느림 개선방법이 있다면 고민해봐야함 (작업내용 1차적으로 보존)\n",
    "* 2만개까지는 heap meomory 에러가 발생하지 않았음 따라서 2만개 이상일때는 분할작업진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T05:42:19.685921Z",
     "start_time": "2020-06-11T05:42:18.034659Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:217: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm,tqdm_notebook  # 진행과정 시각화\n",
    "tqdm.pandas() #apply사용\n",
    "from datetime import timedelta  # 시간날짜\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import math \n",
    "\n",
    "import wordcloud\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Komoran,Okt,Kkma,Twitter  # 자연어처리\n",
    "# from eunjeon import Mecab #은전한닢\n",
    "okt = Okt(max_heap_size=5120)\n",
    "\n",
    "import ckonlpy\n",
    "tw = ckonlpy.tag.Twitter()\n",
    "new_noun = pd.read_excel('단어사전.xlsx')['단어'].to_list()\n",
    "tw.add_dictionary(new_noun,'Noun')\n",
    "\n",
    "#한글깨짐방지\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<style>.container {width:100% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T05:42:19.695914Z",
     "start_time": "2020-06-11T05:42:19.686920Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tokenized(dataframe, keyword, stopword, batch_size = 10000):\n",
    "    '''\n",
    "    dataframe : 분석할 df\n",
    "    keyword : sw에 추가하기 위함\n",
    "    '''\n",
    "\n",
    "    # stopword에 검색어 추가\n",
    "    stopword.append(keyword.split(' ')[0])\n",
    "    \n",
    "    start = 0\n",
    "    end = len(dataframe)\n",
    "    \n",
    "    print(keyword)\n",
    "    if end<=batch_size:    \n",
    "        # tqdm.pandas(desc = '토큰화')\n",
    "        token_df = dataframe.iloc[start:end]['full_text'].progress_apply(lambda x: tw.pos(x, norm=True, stem=True))\n",
    "        token_df_e = token_df.explode()\n",
    "        # tqdm.pandas(desc = '단어추출')\n",
    "        token_noun = token_df_e.apply(lambda x: x[0] if x[1] == 'Noun' else np.nan).dropna()\n",
    "        token_adj = token_df_e.apply(lambda x: x[0] if x[1] == 'Adjective' else np.nan).dropna()\n",
    "\n",
    "        # 클리닝\n",
    "        # tqdm.pandas(desc = '클리닝')\n",
    "        token_noun = token_noun[token_noun.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "        token_adj = token_adj[token_adj.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "        \n",
    "        #저장\n",
    "        main_df = pd.merge(dataframe, token_noun.groupby(level=0).agg(list).rename(\n",
    "            'Noun'), how='left', left_index=True, right_index=True)\n",
    "        main_df = pd.merge(main_df, token_adj.groupby(level=0).agg(list).rename(\n",
    "            'Adjective'), how='left', left_index=True, right_index=True)\n",
    "        main_df.to_csv(f'./output/token_imsi/{keyword}_{start}~{end}.csv')\n",
    "        \n",
    "    else : #20000개 이상인경우 분할 작업\n",
    "        final_end = len(dataframe)\n",
    "        epoch = math.ceil(final_end/batch_size)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while epoch != 0:\n",
    "            # tqdm.pandas(desc = '토큰화')\n",
    "            token_df = dataframe.iloc[start:end]['full_text'].progress_apply(lambda x: tw.pos(x, norm=True, stem=True))\n",
    "            token_df_e = token_df.explode()\n",
    "            # tqdm.pandas(desc = '단어추출')\n",
    "            token_noun = token_df_e.apply(lambda x: x[0] if x[1] == 'Noun' else np.nan).dropna()\n",
    "            token_adj = token_df_e.apply(lambda x: x[0] if x[1] == 'Adjective' else np.nan).dropna()\n",
    "\n",
    "            # 클리닝\n",
    "            # tqdm.pandas(desc = '클리닝')\n",
    "            token_noun = token_noun[token_noun.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "            token_adj = token_adj[token_adj.apply(lambda word: (len(word) > 1) & (word not in sw))]\n",
    "\n",
    "            #저장\n",
    "            main_df = pd.merge(dataframe.iloc[start:end], token_noun.groupby(level=0).agg(list).rename(\n",
    "                'Noun'), how='left', left_index=True, right_index=True)\n",
    "            main_df = pd.merge(main_df, token_adj.groupby(level=0).agg(list).rename(\n",
    "                'Adjective'), how='left', left_index=True, right_index=True)\n",
    "            os.makedirs(f'./output/token_imsi/{keyword}',exist_ok=True)\n",
    "            main_df.to_csv(f'./output/token_imsi/{keyword}/{keyword}_{start}~{end}.csv')\n",
    "                   \n",
    "            #next turn\n",
    "            if epoch >2 :\n",
    "                start += batch_size\n",
    "                end += batch_size\n",
    "            else :      \n",
    "                start += batch_size\n",
    "                end = final_end\n",
    "            \n",
    "            epoch -= 1\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### heap 메모리 부족 오류 발생\n",
    "* heap메모리를 4GB부여하고, 15000개 단위로 분할하여 작업진행 (2만은 가끔 메모리터짐)\n",
    "* 저장시 기본사항을 붙일수 있게 작업\n",
    "* 진행이 길어질수록 속도감소(메모리문제로 추정)하기 때문에 재실행필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T06:08:05.009198Z",
     "start_time": "2020-06-11T05:42:56.894006Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee10713e7b8a41378bad62ee9ad6edd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/634 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하광정리 +강원도\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 634/634 [00:27<00:00, 22.68it/s]\n",
      "  0%|                                                                                | 8/10000 [00:00<03:30, 47.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하조대 +강원도\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [06:32<00:00, 25.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1099/1099 [00:44<00:00, 24.81it/s]\n",
      "  0%|                                                                                | 6/10000 [00:00<03:38, 45.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하조대 +양양\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [05:59<00:00, 27.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3866/3866 [02:40<00:00, 24.03it/s]\n",
      "  0%|                                                                                 | 3/5836 [00:00<03:23, 28.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하조대해수욕장 +양양\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5836/5836 [04:17<00:00, 22.70it/s]\n",
      "  0%|▎                                                                                 | 2/572 [00:00<00:39, 14.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "후진항 +양양\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 572/572 [00:23<00:00, 24.54it/s]\n",
      "  0%|▏                                                                               | 10/5511 [00:00<01:07, 81.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "휴휴암 +양양\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5511/5511 [03:19<00:00, 27.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#임시 속초/속초해수욕장\n",
    "sw = list(pd.read_excel(\"stopword(cp949).xlsx\",encoding = 'cp949')['불용어']) #불용어 불러오기\n",
    "path = 'D:/Python/블로그크롤링/output/크롤링/통합/'\n",
    "file_list = os.listdir(path)\n",
    "for file in tqdm_notebook(file_list[38:]):\n",
    "    file_name = file.split('.')[0]\n",
    "    main_df = pd.read_csv(path + file)\n",
    "    keyword = file.split('_')[0]\n",
    "    tokenized(main_df, keyword, sw)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T05:42:44.768585Z",
     "start_time": "2020-06-11T05:42:44.765577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['하광정리 +강원도_통합_634.csv',\n",
       " '하조대 +강원도_통합_11099.csv',\n",
       " '하조대 +양양_통합_13866.csv',\n",
       " '하조대해수욕장 +양양_통합_5836.csv',\n",
       " '후진항 +양양_통합_572.csv',\n",
       " '휴휴암 +양양_통합_5511.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list[38:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-11T04:32:27.963Z"
    }
   },
   "outputs": [],
   "source": [
    "# file = '광진 +양양_통합_5044.csv'\n",
    "# file_name = file.split('.')[0]\n",
    "# main_df = pd.read_csv(path + file)\n",
    "# keyword = file.split('_')[0]\n",
    "# tokenized(main_df,file_name,keyword,sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-11T04:32:27.964Z"
    }
   },
   "outputs": [],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
